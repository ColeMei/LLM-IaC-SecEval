{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e32593d",
   "metadata": {},
   "source": [
    "# 🤖 LLM Post-Filter Experiment: GLITCH+LLM Pipeline\n",
    "\n",
    "**Focus**: Evaluate **GLITCH + LLM** hybrid approach vs **GLITCH-only** baseline.\n",
    "\n",
    "## 🔬 Experiment Pipeline:\n",
    "\n",
    "1. **Data Preparation**: GLITCH detections + context extracted *(01_data_extraction.py)*\n",
    "2. **LLM Filtering**: Apply GPT-4o mini post-filtering  \n",
    "3. **Performance Evaluation**: Calculate precision/recall improvements\n",
    "\n",
    "## 🎯 Expected Outcomes:\n",
    "- **Precision**: 50-300% improvement\n",
    "- **Recall**: >90% retention  \n",
    "- **FP Reduction**: Significant decrease in false alarms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52b23a",
   "metadata": {},
   "source": [
    "## 🔧 Setup and Configuration\n",
    "\n",
    "**Required**: Set OpenAI API key: `export OPENAI_API_KEY=\"your-key\"`\n",
    "\n",
    "**Model**: GPT-4o mini for cost-effective evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5801de0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏠 Project root: /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval\n",
      "📍 Working directory: /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm_postfilter/notebooks\n",
      "✅ OpenAI API key found (length: 164)\n",
      "🚀 LLM Post-Filter Experiment Ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add llm-postfilter modules to path\n",
    "project_root = Path.cwd().parent.parent.parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "# Import llm-postfilter pipeline components\n",
    "from llm_postfilter import (\n",
    "    GLITCHLLMFilter, \n",
    "    HybridEvaluator,\n",
    "    SecuritySmellPrompts,\n",
    "    SecuritySmell\n",
    ")\n",
    "\n",
    "print(f\"🏠 Project root: {project_root}\")\n",
    "print(f\"📍 Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Check OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"✅ OpenAI API key found (length: {len(api_key)})\")\n",
    "else:\n",
    "    print(\"❌ OpenAI API key not found - set OPENAI_API_KEY environment variable\")\n",
    "\n",
    "print(\"🚀 LLM Post-Filter Experiment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa31952a",
   "metadata": {},
   "source": [
    "## 📁 Step 1: Load Context-Enhanced Data\n",
    "\n",
    "**Load detection files with code context prepared by 01_data_extraction.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155fabcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Found 6 context-enhanced files:\n",
      "  📄 puppet_suspicious_comment_detections_with_context.csv: 23 detections (9 TP, 14 FP, 23 with context)\n",
      "  📄 puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv: 7 detections (4 TP, 3 FP, 7 with context)\n",
      "  📄 puppet_hard_coded_secret_detections_with_context.csv: 66 detections (9 TP, 57 FP, 66 with context)\n",
      "  📄 chef_use_of_weak_cryptography_algorithms_detections_with_context.csv: 2 detections (1 TP, 1 FP, 2 with context)\n",
      "  📄 chef_hard_coded_secret_detections_with_context.csv: 46 detections (9 TP, 37 FP, 46 with context)\n",
      "  📄 chef_suspicious_comment_detections_with_context.csv: 10 detections (4 TP, 6 FP, 10 with context)\n",
      "\n",
      "✅ Context-enhanced data ready for LLM analysis\n"
     ]
    }
   ],
   "source": [
    "# Setup directories\n",
    "data_dir = project_root / \"experiments/llm_postfilter/data\"\n",
    "context_dir = data_dir / \"with_context\"\n",
    "\n",
    "# Find context-enhanced files\n",
    "context_enhanced_files = list(context_dir.glob(\"*_with_context.csv\"))\n",
    "\n",
    "if context_enhanced_files:\n",
    "    print(f\"📁 Found {len(context_enhanced_files)} context-enhanced files:\")\n",
    "    for file in context_enhanced_files:\n",
    "        df = pd.read_csv(file)\n",
    "        tp_count = df['is_true_positive'].sum()\n",
    "        fp_count = len(df) - tp_count\n",
    "        context_success = df['context_success'].sum()\n",
    "        print(f\"  📄 {file.name}: {len(df)} detections ({tp_count} TP, {fp_count} FP, {context_success} with context)\")\n",
    "    \n",
    "    print(f\"\\n✅ Context-enhanced data ready for LLM analysis\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No context-enhanced files found!\")\n",
    "    print(\"➡️  Run 01_data_extraction.py first to prepare the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79241f",
   "metadata": {},
   "source": [
    "## 📝 Step 2: Review LLM Prompt Design\n",
    "\n",
    "Review the formal security smell definitions used for LLM evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449ee4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Security Smell Definitions for LLM\n",
      "========================================\n",
      "\n",
      "📌 Hard-coded secret\n",
      "  A hard-coded secret is a security vulnerability where sensitive information such as passwords, API keys, tokens, certificates, or other credentials are directly embedded in the source code as literal strings or variables, rather than being securely stored and retrieved from external configuration systems, environment variables, or secret management services.\n",
      "  \n",
      "  Key characteristics:\n",
      "  ... (128 words total)\n",
      "\n",
      "📌 Suspicious comment\n",
      "  A suspicious comment is a code comment that indicates potential security issues, incomplete security implementations, or areas requiring security attention. These comments often signal unfinished work, security bypasses, or acknowledged vulnerabilities that may pose risks.\n",
      "  \n",
      "  Key characteristics:\n",
      "  ... (119 words total)\n",
      "\n",
      "📌 Use of weak cryptography algorithms\n",
      "  Use of weak cryptography algorithms refers to the implementation or configuration of cryptographic functions that are known to be vulnerable, deprecated, or insufficient for current security standards. This includes both algorithmically weak ciphers and poor cryptographic practices.\n",
      "  \n",
      "  Key characteristics:\n",
      "  ... (133 words total)\n",
      "\n",
      "✅ 3 smell categories with formal definitions ready\n"
     ]
    }
   ],
   "source": [
    "# Display formal definitions for each security smell\n",
    "print(\"📝 Security Smell Definitions for LLM\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for smell in SecuritySmell:\n",
    "    definition = SecuritySmellPrompts.DEFINITIONS[smell]\n",
    "    lines = definition.strip().split('\\n')[:3]\n",
    "    print(f\"\\n📌 {smell.value}\")\n",
    "    for line in lines:\n",
    "        print(f\"  {line}\")\n",
    "    print(f\"  ... ({len(definition.split())} words total)\")\n",
    "\n",
    "print(f\"\\n✅ {len(SecuritySmell)} smell categories with formal definitions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf30b6d",
   "metadata": {},
   "source": [
    "## 🔧 Step 3: Initialize LLM Pipeline\n",
    "\n",
    "Setup GLITCH+LLM hybrid detection pipeline with GPT-4o mini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89029028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 15:55:10,062 - llm_postfilter.llm_client - INFO - Initialized GPT-4o mini client with model: gpt-4o-mini\n",
      "2025-08-06 15:55:10,062 - llm_postfilter.llm_filter - INFO - Initialized GLITCH+LLM filter pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing GLITCH+LLM pipeline...\n",
      "✅ Pipeline ready:\n",
      "  🤖 Model: gpt-4o-mini\n",
      "  📊 Results → /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm_postfilter/data/llm_results\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM filter pipeline\n",
    "if api_key:\n",
    "    print(\"🔧 Initializing GLITCH+LLM pipeline...\")\n",
    "    \n",
    "    # Create components\n",
    "    llm_filter = GLITCHLLMFilter(\n",
    "        project_root=project_root,\n",
    "        api_key=api_key,\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    evaluator = HybridEvaluator(project_root)\n",
    "    \n",
    "    # Setup directories\n",
    "    results_dir = data_dir / \"llm_results\"\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"✅ Pipeline ready:\")\n",
    "    print(f\"  🤖 Model: {llm_filter.llm_client.model}\")\n",
    "    print(f\"  📊 Results → {results_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Pipeline initialization failed - API key required\")\n",
    "    print(\"Set OPENAI_API_KEY and restart kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891df435",
   "metadata": {},
   "source": [
    "## 🚀 Step 4: Run LLM Post-Filtering\n",
    "\n",
    "Apply LLM post-filtering to GLITCH detections and measure improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b00380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:39:42,422 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for puppet_suspicious_comment_detections_with_context.csv\n",
      "2025-08-06 00:39:42,423 - hybrid.llm_filter - INFO - Loaded 23 detections from puppet_suspicious_comment_detections_with_context.csv\n",
      "2025-08-06 00:39:42,423 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:39:42,430 - hybrid.context_extractor - INFO - Successfully extracted context for 23/23 detections\n",
      "2025-08-06 00:39:42,430 - hybrid.llm_filter - INFO - Context extraction: 23/23 successful (100.0%)\n",
      "2025-08-06 00:39:42,431 - hybrid.llm_filter - INFO - Generated 23 prompts for LLM evaluation\n",
      "2025-08-06 00:39:42,431 - hybrid.llm_filter - INFO - Starting LLM evaluation of 23 detections...\n",
      "2025-08-06 00:39:42,431 - hybrid.llm_client - INFO - Starting batch evaluation of 23 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing 6 context-enhanced files:\n",
      "  📁 puppet_suspicious_comment_detections_with_context.csv: 23 detections (9 TP, 14 FP)\n",
      "  📁 puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv: 7 detections (4 TP, 3 FP)\n",
      "  📁 puppet_hard_coded_secret_detections_with_context.csv: 66 detections (9 TP, 57 FP)\n",
      "  📁 chef_use_of_weak_cryptography_algorithms_detections_with_context.csv: 2 detections (1 TP, 1 FP)\n",
      "  📁 chef_hard_coded_secret_detections_with_context.csv: 46 detections (9 TP, 37 FP)\n",
      "  📁 chef_suspicious_comment_detections_with_context.csv: 10 detections (4 TP, 6 FP)\n",
      "\n",
      "🚀 Starting LLM post-filtering...\n",
      "\n",
      "🔄 Processing 1/6: puppet_suspicious_comment_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:39:42,998 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:44,533 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:45,307 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:46,648 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:47,170 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:47,172 - hybrid.llm_filter - INFO - LLM progress: 5/23 (21.7%)\n",
      "2025-08-06 00:39:48,277 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:49,113 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:50,302 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:51,355 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:52,059 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:52,063 - hybrid.llm_filter - INFO - LLM progress: 10/23 (43.5%)\n",
      "2025-08-06 00:39:52,063 - hybrid.llm_client - INFO - Progress: 10/23 | YES: 3, NO: 7, ERROR: 0\n",
      "2025-08-06 00:39:53,445 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:54,158 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:55,366 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:56,316 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:57,232 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:57,237 - hybrid.llm_filter - INFO - LLM progress: 15/23 (65.2%)\n",
      "2025-08-06 00:39:58,314 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:59,213 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:00,262 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:01,208 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:02,212 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:02,220 - hybrid.llm_filter - INFO - LLM progress: 20/23 (87.0%)\n",
      "2025-08-06 00:40:02,221 - hybrid.llm_client - INFO - Progress: 20/23 | YES: 3, NO: 17, ERROR: 0\n",
      "2025-08-06 00:40:03,291 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:04,162 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:05,218 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:05,223 - hybrid.llm_filter - INFO - LLM progress: 23/23 (100.0%)\n",
      "2025-08-06 00:40:05,224 - hybrid.llm_client - INFO - Progress: 23/23 | YES: 3, NO: 20, ERROR: 0\n",
      "2025-08-06 00:40:05,225 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:40:05,225 - hybrid.llm_filter - INFO -   YES: 3, NO: 20\n",
      "2025-08-06 00:40:05,226 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:40:05,226 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:40:05,227 - hybrid.llm_filter - INFO -   Total time: 22.8s\n",
      "2025-08-06 00:40:05,227 - hybrid.llm_filter - INFO -   Estimated cost: $0.0032\n",
      "2025-08-06 00:40:05,237 - hybrid.llm_filter - INFO - LLM filtering: keeping 3/23 detections (13.0%)\n",
      "2025-08-06 00:40:05,244 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_suspicious_comment_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:40:05,246 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_suspicious_comment_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:40:05,246 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_suspicious_comment_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:40:05,249 - hybrid.llm_filter - INFO - Saved 23 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_suspicious_comment_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:40:05,249 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv\n",
      "2025-08-06 00:40:05,253 - hybrid.llm_filter - INFO - Loaded 7 detections from puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv\n",
      "2025-08-06 00:40:05,253 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:40:05,258 - hybrid.context_extractor - INFO - Successfully extracted context for 7/7 detections\n",
      "2025-08-06 00:40:05,258 - hybrid.llm_filter - INFO - Context extraction: 7/7 successful (100.0%)\n",
      "2025-08-06 00:40:05,258 - hybrid.llm_filter - INFO - Generated 7 prompts for LLM evaluation\n",
      "2025-08-06 00:40:05,258 - hybrid.llm_filter - INFO - Starting LLM evaluation of 7 detections...\n",
      "2025-08-06 00:40:05,259 - hybrid.llm_client - INFO - Starting batch evaluation of 7 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kept 3/23 (13.0%) | TP retention: 3/9 (33.3%)\n",
      "\n",
      "🔄 Processing 2/6: puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:40:06,449 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:07,110 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:08,188 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:09,185 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:10,473 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:10,508 - hybrid.llm_filter - INFO - LLM progress: 5/7 (71.4%)\n",
      "2025-08-06 00:40:11,147 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:12,869 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:12,872 - hybrid.llm_filter - INFO - LLM progress: 7/7 (100.0%)\n",
      "2025-08-06 00:40:12,873 - hybrid.llm_client - INFO - Progress: 7/7 | YES: 5, NO: 2, ERROR: 0\n",
      "2025-08-06 00:40:12,873 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:40:12,873 - hybrid.llm_filter - INFO -   YES: 5, NO: 2\n",
      "2025-08-06 00:40:12,874 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:40:12,874 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:40:12,875 - hybrid.llm_filter - INFO -   Total time: 7.6s\n",
      "2025-08-06 00:40:12,875 - hybrid.llm_filter - INFO -   Estimated cost: $0.0011\n",
      "2025-08-06 00:40:12,879 - hybrid.llm_filter - INFO - LLM filtering: keeping 5/7 detections (71.4%)\n",
      "2025-08-06 00:40:12,884 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_use_of_weak_cryptography_algorithms_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:40:12,887 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_use_of_weak_cryptography_algorithms_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:40:12,887 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:40:12,889 - hybrid.llm_filter - INFO - Saved 7 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:40:12,889 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for puppet_hard_coded_secret_detections_with_context.csv\n",
      "2025-08-06 00:40:12,891 - hybrid.llm_filter - INFO - Loaded 66 detections from puppet_hard_coded_secret_detections_with_context.csv\n",
      "2025-08-06 00:40:12,891 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:40:12,908 - hybrid.context_extractor - INFO - Successfully extracted context for 66/66 detections\n",
      "2025-08-06 00:40:12,908 - hybrid.llm_filter - INFO - Context extraction: 66/66 successful (100.0%)\n",
      "2025-08-06 00:40:12,910 - hybrid.llm_filter - INFO - Generated 66 prompts for LLM evaluation\n",
      "2025-08-06 00:40:12,910 - hybrid.llm_filter - INFO - Starting LLM evaluation of 66 detections...\n",
      "2025-08-06 00:40:12,910 - hybrid.llm_client - INFO - Starting batch evaluation of 66 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kept 5/7 (71.4%) | TP retention: 3/4 (75.0%)\n",
      "\n",
      "🔄 Processing 3/6: puppet_hard_coded_secret_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:40:13,369 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:14,990 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:16,702 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:25,972 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:26,514 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:26,528 - hybrid.llm_filter - INFO - LLM progress: 5/66 (7.6%)\n",
      "2025-08-06 00:40:27,550 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:28,474 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:29,493 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:30,506 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:31,417 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:31,421 - hybrid.llm_filter - INFO - LLM progress: 10/66 (15.2%)\n",
      "2025-08-06 00:40:31,421 - hybrid.llm_client - INFO - Progress: 10/66 | YES: 9, NO: 1, ERROR: 0\n",
      "2025-08-06 00:40:32,389 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:33,592 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:34,453 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:35,457 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:36,438 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:36,440 - hybrid.llm_filter - INFO - LLM progress: 15/66 (22.7%)\n",
      "2025-08-06 00:40:37,502 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:38,535 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:39,627 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:40,657 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:41,475 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:41,476 - hybrid.llm_filter - INFO - LLM progress: 20/66 (30.3%)\n",
      "2025-08-06 00:40:41,476 - hybrid.llm_client - INFO - Progress: 20/66 | YES: 9, NO: 11, ERROR: 0\n",
      "2025-08-06 00:40:42,495 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:43,524 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:44,547 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:45,571 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:46,431 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:46,433 - hybrid.llm_filter - INFO - LLM progress: 25/66 (37.9%)\n",
      "2025-08-06 00:40:47,723 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:48,542 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:49,539 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:50,692 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:51,718 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:51,721 - hybrid.llm_filter - INFO - LLM progress: 30/66 (45.5%)\n",
      "2025-08-06 00:40:51,722 - hybrid.llm_client - INFO - Progress: 30/66 | YES: 10, NO: 20, ERROR: 0\n",
      "2025-08-06 00:40:52,786 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:53,560 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:54,480 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:55,659 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:56,632 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:56,636 - hybrid.llm_filter - INFO - LLM progress: 35/66 (53.0%)\n",
      "2025-08-06 00:40:57,655 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:58,462 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:59,909 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:00,623 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:01,752 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:01,754 - hybrid.llm_filter - INFO - LLM progress: 40/66 (60.6%)\n",
      "2025-08-06 00:41:01,755 - hybrid.llm_client - INFO - Progress: 40/66 | YES: 12, NO: 28, ERROR: 0\n",
      "2025-08-06 00:41:02,672 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:03,792 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:04,824 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:05,611 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:06,505 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:06,509 - hybrid.llm_filter - INFO - LLM progress: 45/66 (68.2%)\n",
      "2025-08-06 00:41:07,588 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:08,839 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:09,739 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:10,726 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:12,373 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:12,390 - hybrid.llm_filter - INFO - LLM progress: 50/66 (75.8%)\n",
      "2025-08-06 00:41:12,392 - hybrid.llm_client - INFO - Progress: 50/66 | YES: 13, NO: 37, ERROR: 0\n",
      "2025-08-06 00:41:13,199 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:13,826 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:15,302 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:15,931 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:17,077 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:17,092 - hybrid.llm_filter - INFO - LLM progress: 55/66 (83.3%)\n",
      "2025-08-06 00:41:18,444 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:18,904 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:19,835 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:21,414 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:22,230 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:22,232 - hybrid.llm_filter - INFO - LLM progress: 60/66 (90.9%)\n",
      "2025-08-06 00:41:22,232 - hybrid.llm_client - INFO - Progress: 60/66 | YES: 14, NO: 46, ERROR: 0\n",
      "2025-08-06 00:41:23,153 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:23,840 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:25,201 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:25,916 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:26,943 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:26,946 - hybrid.llm_filter - INFO - LLM progress: 65/66 (98.5%)\n",
      "2025-08-06 00:41:27,967 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:27,971 - hybrid.llm_filter - INFO - LLM progress: 66/66 (100.0%)\n",
      "2025-08-06 00:41:27,972 - hybrid.llm_client - INFO - Progress: 66/66 | YES: 14, NO: 52, ERROR: 0\n",
      "2025-08-06 00:41:27,972 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:41:27,973 - hybrid.llm_filter - INFO -   YES: 14, NO: 52\n",
      "2025-08-06 00:41:27,974 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:41:27,974 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:41:27,975 - hybrid.llm_filter - INFO -   Total time: 75.0s\n",
      "2025-08-06 00:41:27,975 - hybrid.llm_filter - INFO -   Estimated cost: $0.0098\n",
      "2025-08-06 00:41:27,983 - hybrid.llm_filter - INFO - LLM filtering: keeping 14/66 detections (21.2%)\n",
      "2025-08-06 00:41:27,987 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_hard_coded_secret_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:41:27,989 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_hard_coded_secret_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:41:27,989 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_hard_coded_secret_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:41:27,996 - hybrid.llm_filter - INFO - Saved 66 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_hard_coded_secret_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:41:27,996 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for chef_use_of_weak_cryptography_algorithms_detections_with_context.csv\n",
      "2025-08-06 00:41:27,998 - hybrid.llm_filter - INFO - Loaded 2 detections from chef_use_of_weak_cryptography_algorithms_detections_with_context.csv\n",
      "2025-08-06 00:41:27,998 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:41:28,001 - hybrid.context_extractor - INFO - Successfully extracted context for 2/2 detections\n",
      "2025-08-06 00:41:28,002 - hybrid.llm_filter - INFO - Context extraction: 2/2 successful (100.0%)\n",
      "2025-08-06 00:41:28,002 - hybrid.llm_filter - INFO - Generated 2 prompts for LLM evaluation\n",
      "2025-08-06 00:41:28,002 - hybrid.llm_filter - INFO - Starting LLM evaluation of 2 detections...\n",
      "2025-08-06 00:41:28,002 - hybrid.llm_client - INFO - Starting batch evaluation of 2 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kept 14/66 (21.2%) | TP retention: 9/9 (100.0%)\n",
      "\n",
      "🔄 Processing 4/6: chef_use_of_weak_cryptography_algorithms_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:41:28,911 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:30,016 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:30,020 - hybrid.llm_filter - INFO - LLM progress: 2/2 (100.0%)\n",
      "2025-08-06 00:41:30,021 - hybrid.llm_client - INFO - Progress: 2/2 | YES: 1, NO: 1, ERROR: 0\n",
      "2025-08-06 00:41:30,022 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:41:30,022 - hybrid.llm_filter - INFO -   YES: 1, NO: 1\n",
      "2025-08-06 00:41:30,023 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:41:30,024 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:41:30,025 - hybrid.llm_filter - INFO -   Total time: 2.0s\n",
      "2025-08-06 00:41:30,025 - hybrid.llm_filter - INFO -   Estimated cost: $0.0003\n",
      "2025-08-06 00:41:30,029 - hybrid.llm_filter - INFO - LLM filtering: keeping 1/2 detections (50.0%)\n",
      "2025-08-06 00:41:30,033 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_use_of_weak_cryptography_algorithms_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:41:30,035 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_use_of_weak_cryptography_algorithms_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:41:30,035 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:41:30,036 - hybrid.llm_filter - INFO - Saved 2 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:41:30,037 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for chef_hard_coded_secret_detections_with_context.csv\n",
      "2025-08-06 00:41:30,039 - hybrid.llm_filter - INFO - Loaded 46 detections from chef_hard_coded_secret_detections_with_context.csv\n",
      "2025-08-06 00:41:30,039 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:41:30,050 - hybrid.context_extractor - INFO - Successfully extracted context for 46/46 detections\n",
      "2025-08-06 00:41:30,050 - hybrid.llm_filter - INFO - Context extraction: 46/46 successful (100.0%)\n",
      "2025-08-06 00:41:30,052 - hybrid.llm_filter - INFO - Generated 46 prompts for LLM evaluation\n",
      "2025-08-06 00:41:30,052 - hybrid.llm_filter - INFO - Starting LLM evaluation of 46 detections...\n",
      "2025-08-06 00:41:30,052 - hybrid.llm_client - INFO - Starting batch evaluation of 46 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kept 1/2 (50.0%) | TP retention: 1/1 (100.0%)\n",
      "\n",
      "🔄 Processing 5/6: chef_hard_coded_secret_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:41:30,965 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:32,079 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:33,061 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:33,973 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:35,560 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:35,565 - hybrid.llm_filter - INFO - LLM progress: 5/46 (10.9%)\n",
      "2025-08-06 00:41:36,106 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:37,161 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:38,070 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:39,037 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:40,153 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:40,157 - hybrid.llm_filter - INFO - LLM progress: 10/46 (21.7%)\n",
      "2025-08-06 00:41:40,158 - hybrid.llm_client - INFO - Progress: 10/46 | YES: 3, NO: 7, ERROR: 0\n",
      "2025-08-06 00:41:41,279 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:42,090 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:43,016 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:44,453 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:45,272 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:45,275 - hybrid.llm_filter - INFO - LLM progress: 15/46 (32.6%)\n",
      "2025-08-06 00:41:46,295 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:47,320 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:48,138 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:49,061 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:50,394 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:50,398 - hybrid.llm_filter - INFO - LLM progress: 20/46 (43.5%)\n",
      "2025-08-06 00:41:50,398 - hybrid.llm_client - INFO - Progress: 20/46 | YES: 3, NO: 17, ERROR: 0\n",
      "2025-08-06 00:41:51,417 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:52,104 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:53,081 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:54,081 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:55,103 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:55,107 - hybrid.llm_filter - INFO - LLM progress: 25/46 (54.3%)\n",
      "2025-08-06 00:41:56,129 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:57,088 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:58,226 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:59,277 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:00,121 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:00,124 - hybrid.llm_filter - INFO - LLM progress: 30/46 (65.2%)\n",
      "2025-08-06 00:42:00,125 - hybrid.llm_client - INFO - Progress: 30/46 | YES: 4, NO: 26, ERROR: 0\n",
      "2025-08-06 00:42:01,208 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:02,101 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:03,085 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:04,122 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:05,375 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:05,387 - hybrid.llm_filter - INFO - LLM progress: 35/46 (76.1%)\n",
      "2025-08-06 00:42:06,131 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:07,183 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:08,415 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:09,334 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:10,361 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:10,364 - hybrid.llm_filter - INFO - LLM progress: 40/46 (87.0%)\n",
      "2025-08-06 00:42:10,364 - hybrid.llm_client - INFO - Progress: 40/46 | YES: 4, NO: 36, ERROR: 0\n",
      "2025-08-06 00:42:11,179 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:12,216 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:13,228 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:14,663 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:15,114 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:15,119 - hybrid.llm_filter - INFO - LLM progress: 45/46 (97.8%)\n",
      "2025-08-06 00:42:16,138 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:16,142 - hybrid.llm_filter - INFO - LLM progress: 46/46 (100.0%)\n",
      "2025-08-06 00:42:16,142 - hybrid.llm_client - INFO - Progress: 46/46 | YES: 5, NO: 41, ERROR: 0\n",
      "2025-08-06 00:42:16,143 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:42:16,144 - hybrid.llm_filter - INFO -   YES: 5, NO: 41\n",
      "2025-08-06 00:42:16,144 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:42:16,144 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:42:16,145 - hybrid.llm_filter - INFO -   Total time: 46.1s\n",
      "2025-08-06 00:42:16,145 - hybrid.llm_filter - INFO -   Estimated cost: $0.0076\n",
      "2025-08-06 00:42:16,150 - hybrid.llm_filter - INFO - LLM filtering: keeping 5/46 detections (10.9%)\n",
      "2025-08-06 00:42:16,153 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_hard_coded_secret_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:42:16,155 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_hard_coded_secret_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:42:16,155 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_hard_coded_secret_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:42:16,160 - hybrid.llm_filter - INFO - Saved 46 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_hard_coded_secret_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:42:16,160 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for chef_suspicious_comment_detections_with_context.csv\n",
      "2025-08-06 00:42:16,162 - hybrid.llm_filter - INFO - Loaded 10 detections from chef_suspicious_comment_detections_with_context.csv\n",
      "2025-08-06 00:42:16,162 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:42:16,166 - hybrid.context_extractor - INFO - Successfully extracted context for 10/10 detections\n",
      "2025-08-06 00:42:16,166 - hybrid.llm_filter - INFO - Context extraction: 10/10 successful (100.0%)\n",
      "2025-08-06 00:42:16,166 - hybrid.llm_filter - INFO - Generated 10 prompts for LLM evaluation\n",
      "2025-08-06 00:42:16,167 - hybrid.llm_filter - INFO - Starting LLM evaluation of 10 detections...\n",
      "2025-08-06 00:42:16,167 - hybrid.llm_client - INFO - Starting batch evaluation of 10 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kept 5/46 (10.9%) | TP retention: 3/9 (33.3%)\n",
      "\n",
      "🔄 Processing 6/6: chef_suspicious_comment_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:42:18,402 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:18,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:19,819 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:20,848 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:22,034 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:22,037 - hybrid.llm_filter - INFO - LLM progress: 5/10 (50.0%)\n",
      "2025-08-06 00:42:22,863 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:24,113 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:25,003 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:26,028 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:27,058 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:27,061 - hybrid.llm_filter - INFO - LLM progress: 10/10 (100.0%)\n",
      "2025-08-06 00:42:27,062 - hybrid.llm_client - INFO - Progress: 10/10 | YES: 1, NO: 9, ERROR: 0\n",
      "2025-08-06 00:42:27,062 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:42:27,063 - hybrid.llm_filter - INFO -   YES: 1, NO: 9\n",
      "2025-08-06 00:42:27,063 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:42:27,063 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:42:27,063 - hybrid.llm_filter - INFO -   Total time: 10.9s\n",
      "2025-08-06 00:42:27,064 - hybrid.llm_filter - INFO -   Estimated cost: $0.0014\n",
      "2025-08-06 00:42:27,065 - hybrid.llm_filter - INFO - LLM filtering: keeping 1/10 detections (10.0%)\n",
      "2025-08-06 00:42:27,067 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_suspicious_comment_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:42:27,068 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_suspicious_comment_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:42:27,068 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_suspicious_comment_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:42:27,070 - hybrid.llm_filter - INFO - Saved 10 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_suspicious_comment_detections_with_context_prompts_and_responses.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kept 1/10 (10.0%) | TP retention: 1/4 (25.0%)\n",
      "\n",
      "🎉 LLM filtering completed! Results → /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results\n"
     ]
    }
   ],
   "source": [
    "if api_key and 'context_enhanced_files' in locals():\n",
    "    print(f\"🔍 Processing {len(context_enhanced_files)} context-enhanced files:\")\n",
    "    for file in context_enhanced_files:\n",
    "        df = pd.read_csv(file)\n",
    "        tp_count = df['is_true_positive'].sum()\n",
    "        fp_count = len(df) - tp_count\n",
    "        context_success = df['context_success'].sum()\n",
    "        print(f\"  📁 {file.name}: {len(df)} detections ({tp_count} TP, {fp_count} FP)\")\n",
    "    \n",
    "    print(f\"\\n🚀 Starting LLM post-filtering...\")\n",
    "    \n",
    "    # Process each context-enhanced file\n",
    "    filtered_results = {}\n",
    "    \n",
    "    for i, context_file in enumerate(context_enhanced_files):\n",
    "        print(f\"\\n🔄 Processing {i+1}/{len(context_enhanced_files)}: {context_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Run LLM filtering\n",
    "            filtered_df = llm_filter.filter_detections(context_file, results_dir)\n",
    "            filtered_results[context_file.stem] = filtered_df\n",
    "            \n",
    "            # Summary stats\n",
    "            total = len(filtered_df)\n",
    "            kept = filtered_df['keep_detection'].sum()\n",
    "            original_tp = filtered_df['is_true_positive'].sum() \n",
    "            kept_tp = filtered_df[filtered_df['keep_detection']]['is_true_positive'].sum()\n",
    "            \n",
    "            print(f\"✅ Kept {kept}/{total} ({kept/total:.1%}) | TP retention: {kept_tp}/{original_tp} ({kept_tp/original_tp:.1%})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            logger.error(f\"Failed to process {context_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎉 LLM filtering completed! Results → {results_dir}\")\n",
    "    \n",
    "elif not api_key:\n",
    "    print(\"❌ Skipping - API key required\")\n",
    "else:\n",
    "    print(\"❌ Skipping - no context files (run context extraction first)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5c3aa",
   "metadata": {},
   "source": [
    "## 📈 Step 5: Evaluate Performance Improvement\n",
    "\n",
    "Calculate precision, recall, and F1 improvements from LLM post-filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc577f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:42:27,084 - hybrid.evaluator - INFO - Saved detailed results to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/evaluation/hybrid_evaluation_results.json\n",
      "2025-08-06 00:42:27,086 - hybrid.evaluator - INFO - Saved summary table to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/evaluation/hybrid_evaluation_summary.csv\n",
      "2025-08-06 00:42:27,088 - hybrid.evaluator - INFO - Saved performance comparison to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/evaluation/performance_comparison_table.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluating GLITCH vs GLITCH+LLM Performance\n",
      "==================================================\n",
      "\n",
      "🎯 EXPERIMENT RESULTS\n",
      "========================================\n",
      "\n",
      "📌 chef - Use of weak cryptography algorithms:\n",
      "  Precision: 0.500 → 1.000 (+100.0%)\n",
      "  FP↓: 100.0% | TP retained: 100.0%\n",
      "\n",
      "📌 chef - Hard-coded secret:\n",
      "  Precision: 0.196 → 0.600 (+206.7%)\n",
      "  FP↓: 94.6% | TP retained: 33.3%\n",
      "\n",
      "📌 chef - Suspicious comment:\n",
      "  Precision: 0.400 → 1.000 (+150.0%)\n",
      "  FP↓: 100.0% | TP retained: 25.0%\n",
      "\n",
      "📌 puppet - Suspicious comment:\n",
      "  Precision: 0.391 → 1.000 (+155.6%)\n",
      "  FP↓: 100.0% | TP retained: 33.3%\n",
      "\n",
      "📌 puppet - Use of weak cryptography algorithms:\n",
      "  Precision: 0.571 → 0.600 (+5.0%)\n",
      "  FP↓: 33.3% | TP retained: 75.0%\n",
      "\n",
      "📌 puppet - Hard-coded secret:\n",
      "  Precision: 0.136 → 0.643 (+371.4%)\n",
      "  FP↓: 91.2% | TP retained: 100.0%\n",
      "\n",
      "🚀 OVERALL OUTCOMES:\n",
      "  📈 Precision improvement: +164.8%\n",
      "  📉 FP reduction: 86.5%\n",
      "  🎯 TP retention: 61.1%\n",
      "\n",
      "💾 Detailed results → /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/evaluation\n"
     ]
    }
   ],
   "source": [
    "if api_key and 'filtered_results' in locals():\n",
    "    print(\"📊 Evaluating GLITCH vs GLITCH+LLM Performance\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Organize results by IaC tool\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for tool in ['chef', 'puppet']:\n",
    "        tool_filtered_dfs = []\n",
    "        for key, filtered_df in filtered_results.items():\n",
    "            if key.startswith(tool):\n",
    "                tool_filtered_dfs.append(filtered_df)\n",
    "        \n",
    "        if tool_filtered_dfs:\n",
    "            tool_results = evaluator.evaluate_iac_tool(tool_filtered_dfs, tool.title())\n",
    "            evaluation_results[tool] = tool_results\n",
    "    \n",
    "    # Save evaluation results\n",
    "    evaluation_dir = results_dir / \"evaluation\"\n",
    "    evaluation_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    if evaluation_results:\n",
    "        summary_df = evaluator.save_evaluation_results(evaluation_results, evaluation_dir)\n",
    "        \n",
    "        print(\"\\n🎯 EXPERIMENT RESULTS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Display key findings\n",
    "        for _, row in summary_df.iterrows():\n",
    "            tool = row['IaC_Tool']\n",
    "            smell = row['Security_Smell']\n",
    "            baseline_precision = row['Baseline_Precision']\n",
    "            llm_precision = row['LLM_Precision']\n",
    "            precision_improvement = row['Precision_Improvement']\n",
    "            fp_reduction = row['FP_Reduction']\n",
    "            tp_retention = row['TP_Retention']\n",
    "            \n",
    "            print(f\"\\n📌 {tool} - {smell}:\")\n",
    "            print(f\"  Precision: {baseline_precision:.3f} → {llm_precision:.3f} ({precision_improvement:+.1%})\")\n",
    "            print(f\"  FP↓: {fp_reduction:.1%} | TP retained: {tp_retention:.1%}\")\n",
    "        \n",
    "        # Overall improvements\n",
    "        avg_precision_improvement = summary_df['Precision_Improvement'].mean()\n",
    "        avg_fp_reduction = summary_df['FP_Reduction'].mean()\n",
    "        avg_tp_retention = summary_df['TP_Retention'].mean()\n",
    "        \n",
    "        print(f\"\\n🚀 OVERALL OUTCOMES:\")\n",
    "        print(f\"  📈 Precision improvement: {avg_precision_improvement:+.1%}\")\n",
    "        print(f\"  📉 FP reduction: {avg_fp_reduction:.1%}\")\n",
    "        print(f\"  🎯 TP retention: {avg_tp_retention:.1%}\")\n",
    "        \n",
    "        print(f\"\\n💾 Detailed results → {evaluation_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No evaluation results available\")\n",
    "        \n",
    "else:\n",
    "    print(\"⏭️ Skipping evaluation - run LLM filtering first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224034f6",
   "metadata": {},
   "source": [
    "## 📁 Generated Files & Transparency\n",
    "\n",
    "Complete experimental transparency through intermediate files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7ad639",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Generated Files & Transparency\n",
      "========================================\n",
      "\n",
      "🔍 Context Files (LLM input):\n",
      "  📄 puppet_suspicious_comment_detections_with_context.csv (16 KB)\n",
      "  📄 puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv (5 KB)\n",
      "  📄 puppet_hard_coded_secret_detections_with_context.csv (47 KB)\n",
      "  📄 chef_use_of_weak_cryptography_algorithms_detections_with_context.csv (1 KB)\n",
      "  📄 chef_hard_coded_secret_detections_with_context.csv (37 KB)\n",
      "  📄 chef_suspicious_comment_detections_with_context.csv (7 KB)\n",
      "  📁 /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/detections/with_context\n",
      "\n",
      "🤖 LLM Results:\n",
      "  📊 chef_hard_coded_secret_detections_with_context_llm_filtered.csv (39 KB) - Filtered detections\n",
      "  📊 chef_use_of_weak_cryptography_algorithms_detections_with_context_llm_filtered.csv (1 KB) - Filtered detections\n",
      "  📊 puppet_suspicious_comment_detections_with_context_llm_filtered.csv (17 KB) - Filtered detections\n",
      "  📊 puppet_hard_coded_secret_detections_with_context_llm_filtered.csv (49 KB) - Filtered detections\n",
      "  📊 chef_suspicious_comment_detections_with_context_llm_filtered.csv (8 KB) - Filtered detections\n",
      "  📊 puppet_use_of_weak_cryptography_algorithms_detections_with_context_llm_filtered.csv (6 KB) - Filtered detections\n",
      "  📝 puppet_hard_coded_secret_detections_with_context_prompts_and_responses.json (194 KB) - Full prompts & LLM responses\n",
      "  📝 chef_suspicious_comment_detections_with_context_prompts_and_responses.json (29 KB) - Full prompts & LLM responses\n",
      "  📄 puppet_use_of_weak_cryptography_algorithms_detections_with_context_llm_summary.json (0 KB)\n",
      "  📝 chef_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json (6 KB) - Full prompts & LLM responses\n",
      "  📝 puppet_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json (21 KB) - Full prompts & LLM responses\n",
      "  📄 chef_hard_coded_secret_detections_with_context_llm_summary.json (0 KB)\n",
      "  📄 chef_use_of_weak_cryptography_algorithms_detections_with_context_llm_summary.json (0 KB)\n",
      "  📝 puppet_suspicious_comment_detections_with_context_prompts_and_responses.json (67 KB) - Full prompts & LLM responses\n",
      "  📝 chef_hard_coded_secret_detections_with_context_prompts_and_responses.json (138 KB) - Full prompts & LLM responses\n",
      "  📄 puppet_hard_coded_secret_detections_with_context_llm_summary.json (0 KB)\n",
      "  📄 puppet_suspicious_comment_detections_with_context_llm_summary.json (0 KB)\n",
      "  📄 chef_suspicious_comment_detections_with_context_llm_summary.json (0 KB)\n",
      "  📁 /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results\n",
      "\n",
      "📊 Evaluation:\n",
      "  📄 hybrid_evaluation_summary.csv (1 KB)\n",
      "  📄 performance_comparison_table.csv (0 KB)\n",
      "  📄 hybrid_evaluation_results.json (8 KB)\n",
      "  📁 /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/evaluation\n",
      "\n",
      "💡 Full transparency: code snippets, prompts, LLM decisions, metrics\n"
     ]
    }
   ],
   "source": [
    "print(\"📁 Generated Files & Transparency\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\n🔍 Context Files (LLM input):\")\n",
    "if 'context_dir' in locals():\n",
    "    context_files = list(context_dir.glob(\"*.csv\"))\n",
    "    for file in context_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        print(f\"  📄 {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  📁 {context_dir}\")\n",
    "else:\n",
    "    print(\"  ❌ No context files generated\")\n",
    "\n",
    "print(\"\\n🤖 LLM Results:\")\n",
    "if 'results_dir' in locals() and results_dir.exists():\n",
    "    result_files = list(results_dir.glob(\"*.csv\")) + list(results_dir.glob(\"*.json\"))\n",
    "    for file in result_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        if file.name.endswith(\"_prompts_and_responses.json\"):\n",
    "            print(f\"  📝 {file.name} ({size_kb} KB) - Full prompts & LLM responses\")\n",
    "        elif file.name.endswith(\"_llm_filtered.csv\"):\n",
    "            print(f\"  📊 {file.name} ({size_kb} KB) - Filtered detections\")\n",
    "        else:\n",
    "            print(f\"  📄 {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  📁 {results_dir}\")\n",
    "else:\n",
    "    print(\"  ❌ No LLM results generated\")\n",
    "\n",
    "print(\"\\n📊 Evaluation:\")\n",
    "if 'evaluation_dir' in locals() and evaluation_dir.exists():\n",
    "    eval_files = list(evaluation_dir.glob(\"*.csv\")) + list(evaluation_dir.glob(\"*.json\"))\n",
    "    for file in eval_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        print(f\"  📄 {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  📁 {evaluation_dir}\")\n",
    "else:\n",
    "    print(\"  ❌ No evaluation results generated\")\n",
    "\n",
    "print(\"\\n💡 Full transparency: code snippets, prompts, LLM decisions, metrics\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
