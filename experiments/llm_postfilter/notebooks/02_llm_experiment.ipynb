{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e32593d",
   "metadata": {},
   "source": [
    "# ü§ñ LLM Post-Filter Experiment: GLITCH+LLM Pipeline\n",
    "\n",
    "**Focus**: Evaluate **GLITCH + LLM** hybrid approach vs **GLITCH-only** baseline.\n",
    "\n",
    "## üî¨ Experiment Pipeline:\n",
    "\n",
    "1. **Data Preparation**: GLITCH detections + context extracted *(01_data_extraction.py)*\n",
    "2. **LLM Filtering**: Apply GPT-4o mini post-filtering  \n",
    "3. **Performance Evaluation**: Calculate precision/recall improvements\n",
    "\n",
    "## üéØ Expected Outcomes:\n",
    "- **Precision**: 50-300% improvement\n",
    "- **Recall**: >90% retention  \n",
    "- **FP Reduction**: Significant decrease in false alarms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52b23a",
   "metadata": {},
   "source": [
    "## üîß Setup and Configuration\n",
    "\n",
    "**Required**: Set OpenAI API key: `export OPENAI_API_KEY=\"your-key\"`\n",
    "\n",
    "**Model**: GPT-4o mini for cost-effective evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5801de0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè† Project root: /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval\n",
      "üìç Working directory: /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm_postfilter/notebooks\n",
      "‚úÖ OpenAI API key found (length: 164)\n",
      "üöÄ LLM Post-Filter Experiment Ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add llm-postfilter modules to path\n",
    "project_root = Path.cwd().parent.parent.parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "# Import llm-postfilter pipeline components\n",
    "from llm_postfilter import (\n",
    "    GLITCHLLMFilter, \n",
    "    HybridEvaluator,\n",
    "    SecuritySmellPrompts,\n",
    "    SecuritySmell\n",
    ")\n",
    "\n",
    "print(f\"üè† Project root: {project_root}\")\n",
    "print(f\"üìç Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Check OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"‚úÖ OpenAI API key found (length: {len(api_key)})\")\n",
    "else:\n",
    "    print(\"‚ùå OpenAI API key not found - set OPENAI_API_KEY environment variable\")\n",
    "\n",
    "print(\"üöÄ LLM Post-Filter Experiment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa31952a",
   "metadata": {},
   "source": [
    "## üìÅ Step 1: Load Context-Enhanced Data\n",
    "\n",
    "**Load detection files with code context prepared by 01_data_extraction.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155fabcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 6 context-enhanced files:\n",
      "  üìÑ puppet_suspicious_comment_detections_with_context.csv: 23 detections (9 TP, 14 FP, 23 with context)\n",
      "  üìÑ puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv: 7 detections (4 TP, 3 FP, 7 with context)\n",
      "  üìÑ puppet_hard_coded_secret_detections_with_context.csv: 66 detections (9 TP, 57 FP, 66 with context)\n",
      "  üìÑ chef_use_of_weak_cryptography_algorithms_detections_with_context.csv: 2 detections (1 TP, 1 FP, 2 with context)\n",
      "  üìÑ chef_hard_coded_secret_detections_with_context.csv: 46 detections (9 TP, 37 FP, 46 with context)\n",
      "  üìÑ chef_suspicious_comment_detections_with_context.csv: 10 detections (4 TP, 6 FP, 10 with context)\n",
      "\n",
      "‚úÖ Context-enhanced data ready for LLM analysis\n"
     ]
    }
   ],
   "source": [
    "# Setup directories\n",
    "data_dir = project_root / \"experiments/llm_postfilter/data\"\n",
    "context_dir = data_dir / \"with_context\"\n",
    "\n",
    "# Find context-enhanced files\n",
    "context_enhanced_files = list(context_dir.glob(\"*_with_context.csv\"))\n",
    "\n",
    "if context_enhanced_files:\n",
    "    print(f\"üìÅ Found {len(context_enhanced_files)} context-enhanced files:\")\n",
    "    for file in context_enhanced_files:\n",
    "        df = pd.read_csv(file)\n",
    "        tp_count = df['is_true_positive'].sum()\n",
    "        fp_count = len(df) - tp_count\n",
    "        context_success = df['context_success'].sum()\n",
    "        print(f\"  üìÑ {file.name}: {len(df)} detections ({tp_count} TP, {fp_count} FP, {context_success} with context)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Context-enhanced data ready for LLM analysis\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No context-enhanced files found!\")\n",
    "    print(\"‚û°Ô∏è  Run 01_data_extraction.py first to prepare the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79241f",
   "metadata": {},
   "source": [
    "## üìù Step 2: Review LLM Prompt Design\n",
    "\n",
    "Review the formal security smell definitions used for LLM evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449ee4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Security Smell Definitions for LLM\n",
      "========================================\n",
      "\n",
      "üìå Hard-coded secret\n",
      "  A hard-coded secret is a security vulnerability where sensitive information such as passwords, API keys, tokens, certificates, or other credentials are directly embedded in the source code as literal strings or variables, rather than being securely stored and retrieved from external configuration systems, environment variables, or secret management services.\n",
      "  \n",
      "  Key characteristics:\n",
      "  ... (128 words total)\n",
      "\n",
      "üìå Suspicious comment\n",
      "  A suspicious comment is a code comment that indicates potential security issues, incomplete security implementations, or areas requiring security attention. These comments often signal unfinished work, security bypasses, or acknowledged vulnerabilities that may pose risks.\n",
      "  \n",
      "  Key characteristics:\n",
      "  ... (119 words total)\n",
      "\n",
      "üìå Use of weak cryptography algorithms\n",
      "  Use of weak cryptography algorithms refers to the implementation or configuration of cryptographic functions that are known to be vulnerable, deprecated, or insufficient for current security standards. This includes both algorithmically weak ciphers and poor cryptographic practices.\n",
      "  \n",
      "  Key characteristics:\n",
      "  ... (133 words total)\n",
      "\n",
      "‚úÖ 3 smell categories with formal definitions ready\n"
     ]
    }
   ],
   "source": [
    "# Display formal definitions for each security smell\n",
    "print(\"üìù Security Smell Definitions for LLM\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for smell in SecuritySmell:\n",
    "    definition = SecuritySmellPrompts.DEFINITIONS[smell]\n",
    "    lines = definition.strip().split('\\n')[:3]\n",
    "    print(f\"\\nüìå {smell.value}\")\n",
    "    for line in lines:\n",
    "        print(f\"  {line}\")\n",
    "    print(f\"  ... ({len(definition.split())} words total)\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(SecuritySmell)} smell categories with formal definitions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf30b6d",
   "metadata": {},
   "source": [
    "## üîß Step 3: Initialize LLM Pipeline\n",
    "\n",
    "Setup GLITCH+LLM hybrid detection pipeline with GPT-4o mini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89029028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 15:55:10,062 - llm_postfilter.llm_client - INFO - Initialized GPT-4o mini client with model: gpt-4o-mini\n",
      "2025-08-06 15:55:10,062 - llm_postfilter.llm_filter - INFO - Initialized GLITCH+LLM filter pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing GLITCH+LLM pipeline...\n",
      "‚úÖ Pipeline ready:\n",
      "  ü§ñ Model: gpt-4o-mini\n",
      "  üìä Results ‚Üí /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm_postfilter/data/llm_results\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM filter pipeline\n",
    "if api_key:\n",
    "    print(\"üîß Initializing GLITCH+LLM pipeline...\")\n",
    "    \n",
    "    # Create components\n",
    "    llm_filter = GLITCHLLMFilter(\n",
    "        project_root=project_root,\n",
    "        api_key=api_key,\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    evaluator = HybridEvaluator(project_root)\n",
    "    \n",
    "    # Setup directories\n",
    "    results_dir = data_dir / \"llm_results\"\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"‚úÖ Pipeline ready:\")\n",
    "    print(f\"  ü§ñ Model: {llm_filter.llm_client.model}\")\n",
    "    print(f\"  üìä Results ‚Üí {results_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Pipeline initialization failed - API key required\")\n",
    "    print(\"Set OPENAI_API_KEY and restart kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891df435",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Run LLM Post-Filtering\n",
    "\n",
    "Apply LLM post-filtering to GLITCH detections and measure improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b00380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:39:42,422 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for puppet_suspicious_comment_detections_with_context.csv\n",
      "2025-08-06 00:39:42,423 - hybrid.llm_filter - INFO - Loaded 23 detections from puppet_suspicious_comment_detections_with_context.csv\n",
      "2025-08-06 00:39:42,423 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:39:42,430 - hybrid.context_extractor - INFO - Successfully extracted context for 23/23 detections\n",
      "2025-08-06 00:39:42,430 - hybrid.llm_filter - INFO - Context extraction: 23/23 successful (100.0%)\n",
      "2025-08-06 00:39:42,431 - hybrid.llm_filter - INFO - Generated 23 prompts for LLM evaluation\n",
      "2025-08-06 00:39:42,431 - hybrid.llm_filter - INFO - Starting LLM evaluation of 23 detections...\n",
      "2025-08-06 00:39:42,431 - hybrid.llm_client - INFO - Starting batch evaluation of 23 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 6 context-enhanced files:\n",
      "  üìÅ puppet_suspicious_comment_detections_with_context.csv: 23 detections (9 TP, 14 FP)\n",
      "  üìÅ puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv: 7 detections (4 TP, 3 FP)\n",
      "  üìÅ puppet_hard_coded_secret_detections_with_context.csv: 66 detections (9 TP, 57 FP)\n",
      "  üìÅ chef_use_of_weak_cryptography_algorithms_detections_with_context.csv: 2 detections (1 TP, 1 FP)\n",
      "  üìÅ chef_hard_coded_secret_detections_with_context.csv: 46 detections (9 TP, 37 FP)\n",
      "  üìÅ chef_suspicious_comment_detections_with_context.csv: 10 detections (4 TP, 6 FP)\n",
      "\n",
      "üöÄ Starting LLM post-filtering...\n",
      "\n",
      "üîÑ Processing 1/6: puppet_suspicious_comment_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:39:42,998 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:44,533 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:45,307 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:46,648 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:47,170 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:47,172 - hybrid.llm_filter - INFO - LLM progress: 5/23 (21.7%)\n",
      "2025-08-06 00:39:48,277 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:49,113 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:50,302 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:51,355 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:52,059 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:52,063 - hybrid.llm_filter - INFO - LLM progress: 10/23 (43.5%)\n",
      "2025-08-06 00:39:52,063 - hybrid.llm_client - INFO - Progress: 10/23 | YES: 3, NO: 7, ERROR: 0\n",
      "2025-08-06 00:39:53,445 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:54,158 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:55,366 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:56,316 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:57,232 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:57,237 - hybrid.llm_filter - INFO - LLM progress: 15/23 (65.2%)\n",
      "2025-08-06 00:39:58,314 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:39:59,213 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:00,262 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:01,208 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:02,212 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:02,220 - hybrid.llm_filter - INFO - LLM progress: 20/23 (87.0%)\n",
      "2025-08-06 00:40:02,221 - hybrid.llm_client - INFO - Progress: 20/23 | YES: 3, NO: 17, ERROR: 0\n",
      "2025-08-06 00:40:03,291 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:04,162 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:05,218 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:05,223 - hybrid.llm_filter - INFO - LLM progress: 23/23 (100.0%)\n",
      "2025-08-06 00:40:05,224 - hybrid.llm_client - INFO - Progress: 23/23 | YES: 3, NO: 20, ERROR: 0\n",
      "2025-08-06 00:40:05,225 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:40:05,225 - hybrid.llm_filter - INFO -   YES: 3, NO: 20\n",
      "2025-08-06 00:40:05,226 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:40:05,226 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:40:05,227 - hybrid.llm_filter - INFO -   Total time: 22.8s\n",
      "2025-08-06 00:40:05,227 - hybrid.llm_filter - INFO -   Estimated cost: $0.0032\n",
      "2025-08-06 00:40:05,237 - hybrid.llm_filter - INFO - LLM filtering: keeping 3/23 detections (13.0%)\n",
      "2025-08-06 00:40:05,244 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_suspicious_comment_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:40:05,246 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_suspicious_comment_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:40:05,246 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_suspicious_comment_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:40:05,249 - hybrid.llm_filter - INFO - Saved 23 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_suspicious_comment_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:40:05,249 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv\n",
      "2025-08-06 00:40:05,253 - hybrid.llm_filter - INFO - Loaded 7 detections from puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv\n",
      "2025-08-06 00:40:05,253 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:40:05,258 - hybrid.context_extractor - INFO - Successfully extracted context for 7/7 detections\n",
      "2025-08-06 00:40:05,258 - hybrid.llm_filter - INFO - Context extraction: 7/7 successful (100.0%)\n",
      "2025-08-06 00:40:05,258 - hybrid.llm_filter - INFO - Generated 7 prompts for LLM evaluation\n",
      "2025-08-06 00:40:05,258 - hybrid.llm_filter - INFO - Starting LLM evaluation of 7 detections...\n",
      "2025-08-06 00:40:05,259 - hybrid.llm_client - INFO - Starting batch evaluation of 7 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kept 3/23 (13.0%) | TP retention: 3/9 (33.3%)\n",
      "\n",
      "üîÑ Processing 2/6: puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:40:06,449 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:07,110 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:08,188 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:09,185 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:10,473 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:10,508 - hybrid.llm_filter - INFO - LLM progress: 5/7 (71.4%)\n",
      "2025-08-06 00:40:11,147 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:12,869 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:12,872 - hybrid.llm_filter - INFO - LLM progress: 7/7 (100.0%)\n",
      "2025-08-06 00:40:12,873 - hybrid.llm_client - INFO - Progress: 7/7 | YES: 5, NO: 2, ERROR: 0\n",
      "2025-08-06 00:40:12,873 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:40:12,873 - hybrid.llm_filter - INFO -   YES: 5, NO: 2\n",
      "2025-08-06 00:40:12,874 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:40:12,874 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:40:12,875 - hybrid.llm_filter - INFO -   Total time: 7.6s\n",
      "2025-08-06 00:40:12,875 - hybrid.llm_filter - INFO -   Estimated cost: $0.0011\n",
      "2025-08-06 00:40:12,879 - hybrid.llm_filter - INFO - LLM filtering: keeping 5/7 detections (71.4%)\n",
      "2025-08-06 00:40:12,884 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_use_of_weak_cryptography_algorithms_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:40:12,887 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_use_of_weak_cryptography_algorithms_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:40:12,887 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:40:12,889 - hybrid.llm_filter - INFO - Saved 7 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:40:12,889 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for puppet_hard_coded_secret_detections_with_context.csv\n",
      "2025-08-06 00:40:12,891 - hybrid.llm_filter - INFO - Loaded 66 detections from puppet_hard_coded_secret_detections_with_context.csv\n",
      "2025-08-06 00:40:12,891 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:40:12,908 - hybrid.context_extractor - INFO - Successfully extracted context for 66/66 detections\n",
      "2025-08-06 00:40:12,908 - hybrid.llm_filter - INFO - Context extraction: 66/66 successful (100.0%)\n",
      "2025-08-06 00:40:12,910 - hybrid.llm_filter - INFO - Generated 66 prompts for LLM evaluation\n",
      "2025-08-06 00:40:12,910 - hybrid.llm_filter - INFO - Starting LLM evaluation of 66 detections...\n",
      "2025-08-06 00:40:12,910 - hybrid.llm_client - INFO - Starting batch evaluation of 66 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kept 5/7 (71.4%) | TP retention: 3/4 (75.0%)\n",
      "\n",
      "üîÑ Processing 3/6: puppet_hard_coded_secret_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:40:13,369 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:14,990 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:16,702 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:25,972 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:26,514 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:26,528 - hybrid.llm_filter - INFO - LLM progress: 5/66 (7.6%)\n",
      "2025-08-06 00:40:27,550 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:28,474 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:29,493 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:30,506 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:31,417 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:31,421 - hybrid.llm_filter - INFO - LLM progress: 10/66 (15.2%)\n",
      "2025-08-06 00:40:31,421 - hybrid.llm_client - INFO - Progress: 10/66 | YES: 9, NO: 1, ERROR: 0\n",
      "2025-08-06 00:40:32,389 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:33,592 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:34,453 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:35,457 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:36,438 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:36,440 - hybrid.llm_filter - INFO - LLM progress: 15/66 (22.7%)\n",
      "2025-08-06 00:40:37,502 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:38,535 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:39,627 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:40,657 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:41,475 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:41,476 - hybrid.llm_filter - INFO - LLM progress: 20/66 (30.3%)\n",
      "2025-08-06 00:40:41,476 - hybrid.llm_client - INFO - Progress: 20/66 | YES: 9, NO: 11, ERROR: 0\n",
      "2025-08-06 00:40:42,495 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:43,524 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:44,547 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:45,571 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:46,431 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:46,433 - hybrid.llm_filter - INFO - LLM progress: 25/66 (37.9%)\n",
      "2025-08-06 00:40:47,723 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:48,542 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:49,539 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:50,692 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:51,718 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:51,721 - hybrid.llm_filter - INFO - LLM progress: 30/66 (45.5%)\n",
      "2025-08-06 00:40:51,722 - hybrid.llm_client - INFO - Progress: 30/66 | YES: 10, NO: 20, ERROR: 0\n",
      "2025-08-06 00:40:52,786 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:53,560 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:54,480 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:55,659 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:56,632 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:56,636 - hybrid.llm_filter - INFO - LLM progress: 35/66 (53.0%)\n",
      "2025-08-06 00:40:57,655 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:58,462 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:40:59,909 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:00,623 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:01,752 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:01,754 - hybrid.llm_filter - INFO - LLM progress: 40/66 (60.6%)\n",
      "2025-08-06 00:41:01,755 - hybrid.llm_client - INFO - Progress: 40/66 | YES: 12, NO: 28, ERROR: 0\n",
      "2025-08-06 00:41:02,672 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:03,792 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:04,824 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:05,611 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:06,505 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:06,509 - hybrid.llm_filter - INFO - LLM progress: 45/66 (68.2%)\n",
      "2025-08-06 00:41:07,588 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:08,839 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:09,739 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:10,726 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:12,373 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:12,390 - hybrid.llm_filter - INFO - LLM progress: 50/66 (75.8%)\n",
      "2025-08-06 00:41:12,392 - hybrid.llm_client - INFO - Progress: 50/66 | YES: 13, NO: 37, ERROR: 0\n",
      "2025-08-06 00:41:13,199 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:13,826 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:15,302 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:15,931 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:17,077 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:17,092 - hybrid.llm_filter - INFO - LLM progress: 55/66 (83.3%)\n",
      "2025-08-06 00:41:18,444 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:18,904 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:19,835 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:21,414 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:22,230 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:22,232 - hybrid.llm_filter - INFO - LLM progress: 60/66 (90.9%)\n",
      "2025-08-06 00:41:22,232 - hybrid.llm_client - INFO - Progress: 60/66 | YES: 14, NO: 46, ERROR: 0\n",
      "2025-08-06 00:41:23,153 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:23,840 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:25,201 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:25,916 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:26,943 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:26,946 - hybrid.llm_filter - INFO - LLM progress: 65/66 (98.5%)\n",
      "2025-08-06 00:41:27,967 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:27,971 - hybrid.llm_filter - INFO - LLM progress: 66/66 (100.0%)\n",
      "2025-08-06 00:41:27,972 - hybrid.llm_client - INFO - Progress: 66/66 | YES: 14, NO: 52, ERROR: 0\n",
      "2025-08-06 00:41:27,972 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:41:27,973 - hybrid.llm_filter - INFO -   YES: 14, NO: 52\n",
      "2025-08-06 00:41:27,974 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:41:27,974 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:41:27,975 - hybrid.llm_filter - INFO -   Total time: 75.0s\n",
      "2025-08-06 00:41:27,975 - hybrid.llm_filter - INFO -   Estimated cost: $0.0098\n",
      "2025-08-06 00:41:27,983 - hybrid.llm_filter - INFO - LLM filtering: keeping 14/66 detections (21.2%)\n",
      "2025-08-06 00:41:27,987 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_hard_coded_secret_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:41:27,989 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_hard_coded_secret_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:41:27,989 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_hard_coded_secret_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:41:27,996 - hybrid.llm_filter - INFO - Saved 66 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/puppet_hard_coded_secret_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:41:27,996 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for chef_use_of_weak_cryptography_algorithms_detections_with_context.csv\n",
      "2025-08-06 00:41:27,998 - hybrid.llm_filter - INFO - Loaded 2 detections from chef_use_of_weak_cryptography_algorithms_detections_with_context.csv\n",
      "2025-08-06 00:41:27,998 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:41:28,001 - hybrid.context_extractor - INFO - Successfully extracted context for 2/2 detections\n",
      "2025-08-06 00:41:28,002 - hybrid.llm_filter - INFO - Context extraction: 2/2 successful (100.0%)\n",
      "2025-08-06 00:41:28,002 - hybrid.llm_filter - INFO - Generated 2 prompts for LLM evaluation\n",
      "2025-08-06 00:41:28,002 - hybrid.llm_filter - INFO - Starting LLM evaluation of 2 detections...\n",
      "2025-08-06 00:41:28,002 - hybrid.llm_client - INFO - Starting batch evaluation of 2 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kept 14/66 (21.2%) | TP retention: 9/9 (100.0%)\n",
      "\n",
      "üîÑ Processing 4/6: chef_use_of_weak_cryptography_algorithms_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:41:28,911 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:30,016 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:30,020 - hybrid.llm_filter - INFO - LLM progress: 2/2 (100.0%)\n",
      "2025-08-06 00:41:30,021 - hybrid.llm_client - INFO - Progress: 2/2 | YES: 1, NO: 1, ERROR: 0\n",
      "2025-08-06 00:41:30,022 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:41:30,022 - hybrid.llm_filter - INFO -   YES: 1, NO: 1\n",
      "2025-08-06 00:41:30,023 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:41:30,024 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:41:30,025 - hybrid.llm_filter - INFO -   Total time: 2.0s\n",
      "2025-08-06 00:41:30,025 - hybrid.llm_filter - INFO -   Estimated cost: $0.0003\n",
      "2025-08-06 00:41:30,029 - hybrid.llm_filter - INFO - LLM filtering: keeping 1/2 detections (50.0%)\n",
      "2025-08-06 00:41:30,033 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_use_of_weak_cryptography_algorithms_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:41:30,035 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_use_of_weak_cryptography_algorithms_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:41:30,035 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:41:30,036 - hybrid.llm_filter - INFO - Saved 2 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:41:30,037 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for chef_hard_coded_secret_detections_with_context.csv\n",
      "2025-08-06 00:41:30,039 - hybrid.llm_filter - INFO - Loaded 46 detections from chef_hard_coded_secret_detections_with_context.csv\n",
      "2025-08-06 00:41:30,039 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:41:30,050 - hybrid.context_extractor - INFO - Successfully extracted context for 46/46 detections\n",
      "2025-08-06 00:41:30,050 - hybrid.llm_filter - INFO - Context extraction: 46/46 successful (100.0%)\n",
      "2025-08-06 00:41:30,052 - hybrid.llm_filter - INFO - Generated 46 prompts for LLM evaluation\n",
      "2025-08-06 00:41:30,052 - hybrid.llm_filter - INFO - Starting LLM evaluation of 46 detections...\n",
      "2025-08-06 00:41:30,052 - hybrid.llm_client - INFO - Starting batch evaluation of 46 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kept 1/2 (50.0%) | TP retention: 1/1 (100.0%)\n",
      "\n",
      "üîÑ Processing 5/6: chef_hard_coded_secret_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:41:30,965 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:32,079 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:33,061 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:33,973 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:35,560 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:35,565 - hybrid.llm_filter - INFO - LLM progress: 5/46 (10.9%)\n",
      "2025-08-06 00:41:36,106 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:37,161 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:38,070 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:39,037 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:40,153 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:40,157 - hybrid.llm_filter - INFO - LLM progress: 10/46 (21.7%)\n",
      "2025-08-06 00:41:40,158 - hybrid.llm_client - INFO - Progress: 10/46 | YES: 3, NO: 7, ERROR: 0\n",
      "2025-08-06 00:41:41,279 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:42,090 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:43,016 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:44,453 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:45,272 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:45,275 - hybrid.llm_filter - INFO - LLM progress: 15/46 (32.6%)\n",
      "2025-08-06 00:41:46,295 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:47,320 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:48,138 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:49,061 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:50,394 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:50,398 - hybrid.llm_filter - INFO - LLM progress: 20/46 (43.5%)\n",
      "2025-08-06 00:41:50,398 - hybrid.llm_client - INFO - Progress: 20/46 | YES: 3, NO: 17, ERROR: 0\n",
      "2025-08-06 00:41:51,417 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:52,104 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:53,081 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:54,081 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:55,103 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:55,107 - hybrid.llm_filter - INFO - LLM progress: 25/46 (54.3%)\n",
      "2025-08-06 00:41:56,129 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:57,088 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:58,226 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:41:59,277 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:00,121 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:00,124 - hybrid.llm_filter - INFO - LLM progress: 30/46 (65.2%)\n",
      "2025-08-06 00:42:00,125 - hybrid.llm_client - INFO - Progress: 30/46 | YES: 4, NO: 26, ERROR: 0\n",
      "2025-08-06 00:42:01,208 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:02,101 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:03,085 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:04,122 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:05,375 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:05,387 - hybrid.llm_filter - INFO - LLM progress: 35/46 (76.1%)\n",
      "2025-08-06 00:42:06,131 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:07,183 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:08,415 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:09,334 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:10,361 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:10,364 - hybrid.llm_filter - INFO - LLM progress: 40/46 (87.0%)\n",
      "2025-08-06 00:42:10,364 - hybrid.llm_client - INFO - Progress: 40/46 | YES: 4, NO: 36, ERROR: 0\n",
      "2025-08-06 00:42:11,179 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:12,216 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:13,228 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:14,663 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:15,114 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:15,119 - hybrid.llm_filter - INFO - LLM progress: 45/46 (97.8%)\n",
      "2025-08-06 00:42:16,138 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:16,142 - hybrid.llm_filter - INFO - LLM progress: 46/46 (100.0%)\n",
      "2025-08-06 00:42:16,142 - hybrid.llm_client - INFO - Progress: 46/46 | YES: 5, NO: 41, ERROR: 0\n",
      "2025-08-06 00:42:16,143 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:42:16,144 - hybrid.llm_filter - INFO -   YES: 5, NO: 41\n",
      "2025-08-06 00:42:16,144 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:42:16,144 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:42:16,145 - hybrid.llm_filter - INFO -   Total time: 46.1s\n",
      "2025-08-06 00:42:16,145 - hybrid.llm_filter - INFO -   Estimated cost: $0.0076\n",
      "2025-08-06 00:42:16,150 - hybrid.llm_filter - INFO - LLM filtering: keeping 5/46 detections (10.9%)\n",
      "2025-08-06 00:42:16,153 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_hard_coded_secret_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:42:16,155 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_hard_coded_secret_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:42:16,155 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_hard_coded_secret_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:42:16,160 - hybrid.llm_filter - INFO - Saved 46 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_hard_coded_secret_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:42:16,160 - hybrid.llm_filter - INFO - Starting LLM post-filtering pipeline for chef_suspicious_comment_detections_with_context.csv\n",
      "2025-08-06 00:42:16,162 - hybrid.llm_filter - INFO - Loaded 10 detections from chef_suspicious_comment_detections_with_context.csv\n",
      "2025-08-06 00:42:16,162 - hybrid.llm_filter - INFO - Extracting code context for detections...\n",
      "2025-08-06 00:42:16,166 - hybrid.context_extractor - INFO - Successfully extracted context for 10/10 detections\n",
      "2025-08-06 00:42:16,166 - hybrid.llm_filter - INFO - Context extraction: 10/10 successful (100.0%)\n",
      "2025-08-06 00:42:16,166 - hybrid.llm_filter - INFO - Generated 10 prompts for LLM evaluation\n",
      "2025-08-06 00:42:16,167 - hybrid.llm_filter - INFO - Starting LLM evaluation of 10 detections...\n",
      "2025-08-06 00:42:16,167 - hybrid.llm_client - INFO - Starting batch evaluation of 10 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kept 5/46 (10.9%) | TP retention: 3/9 (33.3%)\n",
      "\n",
      "üîÑ Processing 6/6: chef_suspicious_comment_detections_with_context.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:42:18,402 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:18,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:19,819 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:20,848 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:22,034 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:22,037 - hybrid.llm_filter - INFO - LLM progress: 5/10 (50.0%)\n",
      "2025-08-06 00:42:22,863 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:24,113 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:25,003 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:26,028 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:27,058 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-06 00:42:27,061 - hybrid.llm_filter - INFO - LLM progress: 10/10 (100.0%)\n",
      "2025-08-06 00:42:27,062 - hybrid.llm_client - INFO - Progress: 10/10 | YES: 1, NO: 9, ERROR: 0\n",
      "2025-08-06 00:42:27,062 - hybrid.llm_filter - INFO - LLM evaluation completed:\n",
      "2025-08-06 00:42:27,063 - hybrid.llm_filter - INFO -   YES: 1, NO: 9\n",
      "2025-08-06 00:42:27,063 - hybrid.llm_filter - INFO -   ERROR: 0, UNCLEAR: 0\n",
      "2025-08-06 00:42:27,063 - hybrid.llm_filter - INFO -   Success rate: 100.0%\n",
      "2025-08-06 00:42:27,063 - hybrid.llm_filter - INFO -   Total time: 10.9s\n",
      "2025-08-06 00:42:27,064 - hybrid.llm_filter - INFO -   Estimated cost: $0.0014\n",
      "2025-08-06 00:42:27,065 - hybrid.llm_filter - INFO - LLM filtering: keeping 1/10 detections (10.0%)\n",
      "2025-08-06 00:42:27,067 - hybrid.llm_filter - INFO - Saved filtered detections to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_suspicious_comment_detections_with_context_llm_filtered.csv\n",
      "2025-08-06 00:42:27,068 - hybrid.llm_filter - INFO - Saved analysis summary to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_suspicious_comment_detections_with_context_llm_summary.json\n",
      "2025-08-06 00:42:27,068 - hybrid.llm_filter - INFO - Saving prompts and responses to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_suspicious_comment_detections_with_context_prompts_and_responses.json\n",
      "2025-08-06 00:42:27,070 - hybrid.llm_filter - INFO - Saved 10 prompt/response pairs to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/chef_suspicious_comment_detections_with_context_prompts_and_responses.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kept 1/10 (10.0%) | TP retention: 1/4 (25.0%)\n",
      "\n",
      "üéâ LLM filtering completed! Results ‚Üí /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results\n"
     ]
    }
   ],
   "source": [
    "if api_key and 'context_enhanced_files' in locals():\n",
    "    print(f\"üîç Processing {len(context_enhanced_files)} context-enhanced files:\")\n",
    "    for file in context_enhanced_files:\n",
    "        df = pd.read_csv(file)\n",
    "        tp_count = df['is_true_positive'].sum()\n",
    "        fp_count = len(df) - tp_count\n",
    "        context_success = df['context_success'].sum()\n",
    "        print(f\"  üìÅ {file.name}: {len(df)} detections ({tp_count} TP, {fp_count} FP)\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting LLM post-filtering...\")\n",
    "    \n",
    "    # Process each context-enhanced file\n",
    "    filtered_results = {}\n",
    "    \n",
    "    for i, context_file in enumerate(context_enhanced_files):\n",
    "        print(f\"\\nüîÑ Processing {i+1}/{len(context_enhanced_files)}: {context_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Run LLM filtering\n",
    "            filtered_df = llm_filter.filter_detections(context_file, results_dir)\n",
    "            filtered_results[context_file.stem] = filtered_df\n",
    "            \n",
    "            # Summary stats\n",
    "            total = len(filtered_df)\n",
    "            kept = filtered_df['keep_detection'].sum()\n",
    "            original_tp = filtered_df['is_true_positive'].sum() \n",
    "            kept_tp = filtered_df[filtered_df['keep_detection']]['is_true_positive'].sum()\n",
    "            \n",
    "            print(f\"‚úÖ Kept {kept}/{total} ({kept/total:.1%}) | TP retention: {kept_tp}/{original_tp} ({kept_tp/original_tp:.1%})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            logger.error(f\"Failed to process {context_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ LLM filtering completed! Results ‚Üí {results_dir}\")\n",
    "    \n",
    "elif not api_key:\n",
    "    print(\"‚ùå Skipping - API key required\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping - no context files (run context extraction first)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5c3aa",
   "metadata": {},
   "source": [
    "## üìà Step 5: Evaluate Performance Improvement\n",
    "\n",
    "Calculate precision, recall, and F1 improvements from LLM post-filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc577f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:42:27,084 - hybrid.evaluator - INFO - Saved detailed results to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/evaluation/hybrid_evaluation_results.json\n",
      "2025-08-06 00:42:27,086 - hybrid.evaluator - INFO - Saved summary table to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/evaluation/hybrid_evaluation_summary.csv\n",
      "2025-08-06 00:42:27,088 - hybrid.evaluator - INFO - Saved performance comparison to /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/evaluation/performance_comparison_table.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating GLITCH vs GLITCH+LLM Performance\n",
      "==================================================\n",
      "\n",
      "üéØ EXPERIMENT RESULTS\n",
      "========================================\n",
      "\n",
      "üìå chef - Use of weak cryptography algorithms:\n",
      "  Precision: 0.500 ‚Üí 1.000 (+100.0%)\n",
      "  FP‚Üì: 100.0% | TP retained: 100.0%\n",
      "\n",
      "üìå chef - Hard-coded secret:\n",
      "  Precision: 0.196 ‚Üí 0.600 (+206.7%)\n",
      "  FP‚Üì: 94.6% | TP retained: 33.3%\n",
      "\n",
      "üìå chef - Suspicious comment:\n",
      "  Precision: 0.400 ‚Üí 1.000 (+150.0%)\n",
      "  FP‚Üì: 100.0% | TP retained: 25.0%\n",
      "\n",
      "üìå puppet - Suspicious comment:\n",
      "  Precision: 0.391 ‚Üí 1.000 (+155.6%)\n",
      "  FP‚Üì: 100.0% | TP retained: 33.3%\n",
      "\n",
      "üìå puppet - Use of weak cryptography algorithms:\n",
      "  Precision: 0.571 ‚Üí 0.600 (+5.0%)\n",
      "  FP‚Üì: 33.3% | TP retained: 75.0%\n",
      "\n",
      "üìå puppet - Hard-coded secret:\n",
      "  Precision: 0.136 ‚Üí 0.643 (+371.4%)\n",
      "  FP‚Üì: 91.2% | TP retained: 100.0%\n",
      "\n",
      "üöÄ OVERALL OUTCOMES:\n",
      "  üìà Precision improvement: +164.8%\n",
      "  üìâ FP reduction: 86.5%\n",
      "  üéØ TP retention: 61.1%\n",
      "\n",
      "üíæ Detailed results ‚Üí /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/evaluation\n"
     ]
    }
   ],
   "source": [
    "if api_key and 'filtered_results' in locals():\n",
    "    print(\"üìä Evaluating GLITCH vs GLITCH+LLM Performance\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Organize results by IaC tool\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for tool in ['chef', 'puppet']:\n",
    "        tool_filtered_dfs = []\n",
    "        for key, filtered_df in filtered_results.items():\n",
    "            if key.startswith(tool):\n",
    "                tool_filtered_dfs.append(filtered_df)\n",
    "        \n",
    "        if tool_filtered_dfs:\n",
    "            tool_results = evaluator.evaluate_iac_tool(tool_filtered_dfs, tool.title())\n",
    "            evaluation_results[tool] = tool_results\n",
    "    \n",
    "    # Save evaluation results\n",
    "    evaluation_dir = results_dir / \"evaluation\"\n",
    "    evaluation_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    if evaluation_results:\n",
    "        summary_df = evaluator.save_evaluation_results(evaluation_results, evaluation_dir)\n",
    "        \n",
    "        print(\"\\nüéØ EXPERIMENT RESULTS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Display key findings\n",
    "        for _, row in summary_df.iterrows():\n",
    "            tool = row['IaC_Tool']\n",
    "            smell = row['Security_Smell']\n",
    "            baseline_precision = row['Baseline_Precision']\n",
    "            llm_precision = row['LLM_Precision']\n",
    "            precision_improvement = row['Precision_Improvement']\n",
    "            fp_reduction = row['FP_Reduction']\n",
    "            tp_retention = row['TP_Retention']\n",
    "            \n",
    "            print(f\"\\nüìå {tool} - {smell}:\")\n",
    "            print(f\"  Precision: {baseline_precision:.3f} ‚Üí {llm_precision:.3f} ({precision_improvement:+.1%})\")\n",
    "            print(f\"  FP‚Üì: {fp_reduction:.1%} | TP retained: {tp_retention:.1%}\")\n",
    "        \n",
    "        # Overall improvements\n",
    "        avg_precision_improvement = summary_df['Precision_Improvement'].mean()\n",
    "        avg_fp_reduction = summary_df['FP_Reduction'].mean()\n",
    "        avg_tp_retention = summary_df['TP_Retention'].mean()\n",
    "        \n",
    "        print(f\"\\nüöÄ OVERALL OUTCOMES:\")\n",
    "        print(f\"  üìà Precision improvement: {avg_precision_improvement:+.1%}\")\n",
    "        print(f\"  üìâ FP reduction: {avg_fp_reduction:.1%}\")\n",
    "        print(f\"  üéØ TP retention: {avg_tp_retention:.1%}\")\n",
    "        \n",
    "        print(f\"\\nüíæ Detailed results ‚Üí {evaluation_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No evaluation results available\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping evaluation - run LLM filtering first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224034f6",
   "metadata": {},
   "source": [
    "## üìÅ Generated Files & Transparency\n",
    "\n",
    "Complete experimental transparency through intermediate files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7ad639",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Generated Files & Transparency\n",
      "========================================\n",
      "\n",
      "üîç Context Files (LLM input):\n",
      "  üìÑ puppet_suspicious_comment_detections_with_context.csv (16 KB)\n",
      "  üìÑ puppet_use_of_weak_cryptography_algorithms_detections_with_context.csv (5 KB)\n",
      "  üìÑ puppet_hard_coded_secret_detections_with_context.csv (47 KB)\n",
      "  üìÑ chef_use_of_weak_cryptography_algorithms_detections_with_context.csv (1 KB)\n",
      "  üìÑ chef_hard_coded_secret_detections_with_context.csv (37 KB)\n",
      "  üìÑ chef_suspicious_comment_detections_with_context.csv (7 KB)\n",
      "  üìÅ /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/detections/with_context\n",
      "\n",
      "ü§ñ LLM Results:\n",
      "  üìä chef_hard_coded_secret_detections_with_context_llm_filtered.csv (39 KB) - Filtered detections\n",
      "  üìä chef_use_of_weak_cryptography_algorithms_detections_with_context_llm_filtered.csv (1 KB) - Filtered detections\n",
      "  üìä puppet_suspicious_comment_detections_with_context_llm_filtered.csv (17 KB) - Filtered detections\n",
      "  üìä puppet_hard_coded_secret_detections_with_context_llm_filtered.csv (49 KB) - Filtered detections\n",
      "  üìä chef_suspicious_comment_detections_with_context_llm_filtered.csv (8 KB) - Filtered detections\n",
      "  üìä puppet_use_of_weak_cryptography_algorithms_detections_with_context_llm_filtered.csv (6 KB) - Filtered detections\n",
      "  üìù puppet_hard_coded_secret_detections_with_context_prompts_and_responses.json (194 KB) - Full prompts & LLM responses\n",
      "  üìù chef_suspicious_comment_detections_with_context_prompts_and_responses.json (29 KB) - Full prompts & LLM responses\n",
      "  üìÑ puppet_use_of_weak_cryptography_algorithms_detections_with_context_llm_summary.json (0 KB)\n",
      "  üìù chef_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json (6 KB) - Full prompts & LLM responses\n",
      "  üìù puppet_use_of_weak_cryptography_algorithms_detections_with_context_prompts_and_responses.json (21 KB) - Full prompts & LLM responses\n",
      "  üìÑ chef_hard_coded_secret_detections_with_context_llm_summary.json (0 KB)\n",
      "  üìÑ chef_use_of_weak_cryptography_algorithms_detections_with_context_llm_summary.json (0 KB)\n",
      "  üìù puppet_suspicious_comment_detections_with_context_prompts_and_responses.json (67 KB) - Full prompts & LLM responses\n",
      "  üìù chef_hard_coded_secret_detections_with_context_prompts_and_responses.json (138 KB) - Full prompts & LLM responses\n",
      "  üìÑ puppet_hard_coded_secret_detections_with_context_llm_summary.json (0 KB)\n",
      "  üìÑ puppet_suspicious_comment_detections_with_context_llm_summary.json (0 KB)\n",
      "  üìÑ chef_suspicious_comment_detections_with_context_llm_summary.json (0 KB)\n",
      "  üìÅ /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results\n",
      "\n",
      "üìä Evaluation:\n",
      "  üìÑ hybrid_evaluation_summary.csv (1 KB)\n",
      "  üìÑ performance_comparison_table.csv (0 KB)\n",
      "  üìÑ hybrid_evaluation_results.json (8 KB)\n",
      "  üìÅ /Users/colemei/Library/Mobile Documents/com~apple~CloudDocs/01.Work/04.Master/Course/Research Program/Project/LLM-IaC-SecEval/experiments/llm-postfilter/data/llm_results/evaluation\n",
      "\n",
      "üí° Full transparency: code snippets, prompts, LLM decisions, metrics\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÅ Generated Files & Transparency\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nüîç Context Files (LLM input):\")\n",
    "if 'context_dir' in locals():\n",
    "    context_files = list(context_dir.glob(\"*.csv\"))\n",
    "    for file in context_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        print(f\"  üìÑ {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  üìÅ {context_dir}\")\n",
    "else:\n",
    "    print(\"  ‚ùå No context files generated\")\n",
    "\n",
    "print(\"\\nü§ñ LLM Results:\")\n",
    "if 'results_dir' in locals() and results_dir.exists():\n",
    "    result_files = list(results_dir.glob(\"*.csv\")) + list(results_dir.glob(\"*.json\"))\n",
    "    for file in result_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        if file.name.endswith(\"_prompts_and_responses.json\"):\n",
    "            print(f\"  üìù {file.name} ({size_kb} KB) - Full prompts & LLM responses\")\n",
    "        elif file.name.endswith(\"_llm_filtered.csv\"):\n",
    "            print(f\"  üìä {file.name} ({size_kb} KB) - Filtered detections\")\n",
    "        else:\n",
    "            print(f\"  üìÑ {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  üìÅ {results_dir}\")\n",
    "else:\n",
    "    print(\"  ‚ùå No LLM results generated\")\n",
    "\n",
    "print(\"\\nüìä Evaluation:\")\n",
    "if 'evaluation_dir' in locals() and evaluation_dir.exists():\n",
    "    eval_files = list(evaluation_dir.glob(\"*.csv\")) + list(evaluation_dir.glob(\"*.json\"))\n",
    "    for file in eval_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        print(f\"  üìÑ {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  üìÅ {evaluation_dir}\")\n",
    "else:\n",
    "    print(\"  ‚ùå No evaluation results generated\")\n",
    "\n",
    "print(\"\\nüí° Full transparency: code snippets, prompts, LLM decisions, metrics\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
