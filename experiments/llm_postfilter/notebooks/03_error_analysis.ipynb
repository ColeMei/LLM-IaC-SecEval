{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç LLM Post-Filter Error Analysis\n",
        "\n",
        "**Objective**: Deep dive into misclassified examples to understand why the LLM is losing true positives and keeping false positives.\n",
        "\n",
        "## üéØ Analysis Focus:\n",
        "1. **Lost True Positives** (Type 1 Error): Original TP ‚Üí LLM filtered out (False Negatives)\n",
        "2. **Kept False Positives** (Type 2 Error): Original FP ‚Üí LLM kept (False Positives)\n",
        "\n",
        "## üîß Investigation Questions:\n",
        "- What patterns exist in misclassified code snippets?\n",
        "- Is context window too narrow for certain types?\n",
        "- Are prompt definitions misaligned with IaC-specific patterns?\n",
        "- Which smell categories are most problematic?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import ipywidgets as widgets\n",
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\n",
        "# Set up styling\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "print(\"üîß Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéõÔ∏è Interactive Configuration (Refactored)\n",
        "project_root = Path.cwd().parent.parent.parent\n",
        "results_dir = project_root / \"results/llm_postfilter\"\n",
        "\n",
        "# Helpers\n",
        "def list_experiment_options(base_dir: Path):\n",
        "    folders = [d for d in base_dir.iterdir() if d.is_dir() and d.name.startswith('2025')]\n",
        "    return [(f\"{f.name} ({len(list(f.glob('*.csv')))} files)\", f.name) for f in sorted(folders, reverse=True)]\n",
        "\n",
        "class CheckboxGroup:\n",
        "    def __init__(self, title: str, options, selected=None):\n",
        "        selected = set(selected or [])\n",
        "        self.checkboxes = [\n",
        "            widgets.Checkbox(\n",
        "                value=(opt in selected), description=str(opt), style={'description_width': 'initial'}\n",
        "            ) for opt in options\n",
        "        ]\n",
        "        self.widget = widgets.VBox([widgets.HTML(f\"<b>{title}:</b>\"), *self.checkboxes])\n",
        "\n",
        "    @property\n",
        "    def selected(self):\n",
        "        return [cb.description for cb in self.checkboxes if cb.value]\n",
        "\n",
        "# Base configuration\n",
        "config = {\n",
        "    'experiment_folder': None,\n",
        "    'iac_tools': ['chef', 'puppet'],\n",
        "    'smell_categories': ['Hard-coded secret', 'Suspicious comment', 'Use of weak cryptography algorithms'],\n",
        "    'analysis_mode': 'both',\n",
        "    'max_examples': 10,\n",
        "}\n",
        "\n",
        "# Build UI\n",
        "experiment_options = list_experiment_options(results_dir)\n",
        "experiment_widget = widgets.Dropdown(\n",
        "    options=experiment_options or [('No experiments found', '')],\n",
        "    value=(experiment_options[0][1] if experiment_options else ''),\n",
        "    description='Experiment:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='500px')\n",
        ")\n",
        "\n",
        "iac_group = CheckboxGroup(\"IaC Tools\", ['chef', 'puppet'], selected=config['iac_tools'])\n",
        "smell_group = CheckboxGroup(\"Security Smells\", config['smell_categories'], selected=config['smell_categories'])\n",
        "\n",
        "mode_widget = widgets.RadioButtons(\n",
        "    options=[\n",
        "        ('üî¥ Lost True Positives (TP‚ÜíFN)', 'lost_tp'),\n",
        "        ('üü° Kept False Positives (FP‚ÜíFP)', 'kept_fp'),\n",
        "        ('üîç Both Error Types', 'both')\n",
        "    ],\n",
        "    value=config['analysis_mode'],\n",
        "    description='Analysis Mode:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "max_examples_widget = widgets.IntSlider(\n",
        "    value=config['max_examples'], min=5, max=50, step=5,\n",
        "    description='Max Examples:', style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "def update_config():\n",
        "    config['experiment_folder'] = experiment_widget.value\n",
        "    config['iac_tools'] = iac_group.selected\n",
        "    config['smell_categories'] = smell_group.selected\n",
        "    config['analysis_mode'] = mode_widget.value\n",
        "    config['max_examples'] = int(max_examples_widget.value)\n",
        "    print(\n",
        "        \"‚úÖ Config updated:\",\n",
        "        f\"experiment={config['experiment_folder']}\",\n",
        "        f\"iac_tools={config['iac_tools']}\",\n",
        "        f\"smells={len(config['smell_categories'])}\",\n",
        "        f\"mode={config['analysis_mode']}\",\n",
        "        f\"max_examples={config['max_examples']}\"\n",
        "    )\n",
        "\n",
        "print(\"üéõÔ∏è Configure Error Analysis:\")\n",
        "display(widgets.VBox([experiment_widget, iac_group.widget, smell_group.widget, mode_widget, max_examples_widget]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Data Loading & Overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update configuration\n",
        "update_config()\n",
        "\n",
        "# Load experiment data\n",
        "experiment_path = results_dir / config['experiment_folder']\n",
        "print(f\"üìÇ Loading data from: {experiment_path}\")\n",
        "\n",
        "# Function to load all detection files\n",
        "def load_experiment_data(experiment_path, iac_tools, smell_categories):\n",
        "    all_data = []\n",
        "    \n",
        "    for iac_tool in iac_tools:\n",
        "        for smell in smell_categories:\n",
        "            # Clean smell name for filename\n",
        "            smell_clean = smell.replace(' ', '_').replace('-', '_').lower()\n",
        "            csv_file = experiment_path / f\"{iac_tool}_{smell_clean}_detections_with_context_llm_filtered.csv\"\n",
        "            \n",
        "            if csv_file.exists():\n",
        "                df = pd.read_csv(csv_file)\n",
        "                df['experiment_folder'] = config['experiment_folder']\n",
        "                df['analysis_id'] = f\"{iac_tool}_{smell}\"\n",
        "                all_data.append(df)\n",
        "                print(f\"‚úÖ Loaded {iac_tool} {smell}: {len(df)} detections\")\n",
        "            else:\n",
        "                print(f\"‚ùå Missing: {csv_file.name}\")\n",
        "    \n",
        "    if all_data:\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No data loaded!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load the data\n",
        "df = load_experiment_data(experiment_path, config['iac_tools'], config['smell_categories'])\n",
        "\n",
        "if not df.empty:\n",
        "    print(f\"\\nüìä Total loaded: {len(df)} detections\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "else:\n",
        "    print(\"‚ùå No data to analyze!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df.empty:\n",
        "    # Classification summary\n",
        "    print(\"üîç Classification Overview:\")\n",
        "    \n",
        "    # Define error types\n",
        "    df['error_type'] = 'Correct'\n",
        "    \n",
        "    # Lost True Positives (Type 1 Error): TP but LLM said NO\n",
        "    lost_tp_mask = (df['is_true_positive'] == True) & (df['keep_detection'] == False)\n",
        "    df.loc[lost_tp_mask, 'error_type'] = 'Lost TP'\n",
        "    \n",
        "    # Kept False Positives (Type 2 Error): FP but LLM said YES  \n",
        "    kept_fp_mask = (df['is_true_positive'] == False) & (df['keep_detection'] == True)\n",
        "    df.loc[kept_fp_mask, 'error_type'] = 'Kept FP'\n",
        "    \n",
        "    # Summary statistics\n",
        "    summary = df.groupby(['iac_tool', 'smell_category', 'error_type']).size().unstack(fill_value=0)\n",
        "    display(summary)\n",
        "    \n",
        "    # Error rates by category\n",
        "    print(\"\\nüìà Error Analysis by Category:\")\n",
        "    error_analysis = []\n",
        "    \n",
        "    for (iac_tool, smell), group in df.groupby(['iac_tool', 'smell_category']):\n",
        "        total_tp = (group['is_true_positive'] == True).sum()\n",
        "        total_fp = (group['is_true_positive'] == False).sum()\n",
        "        lost_tp = ((group['is_true_positive'] == True) & (group['keep_detection'] == False)).sum()\n",
        "        kept_fp = ((group['is_true_positive'] == False) & (group['keep_detection'] == True)).sum()\n",
        "        \n",
        "        if total_tp > 0:\n",
        "            tp_loss_rate = lost_tp / total_tp\n",
        "        else:\n",
        "            tp_loss_rate = 0\n",
        "            \n",
        "        if total_fp > 0:\n",
        "            fp_retention_rate = kept_fp / total_fp\n",
        "        else:\n",
        "            fp_retention_rate = 0\n",
        "        \n",
        "        error_analysis.append({\n",
        "            'IaC Tool': iac_tool,\n",
        "            'Security Smell': smell,\n",
        "            'Total TP': total_tp,\n",
        "            'Lost TP': lost_tp,\n",
        "            'TP Loss Rate': f\"{tp_loss_rate:.1%}\",\n",
        "            'Total FP': total_fp,\n",
        "            'Kept FP': kept_fp,\n",
        "            'FP Retention Rate': f\"{fp_retention_rate:.1%}\"\n",
        "        })\n",
        "    \n",
        "    error_df = pd.DataFrame(error_analysis)\n",
        "    display(error_df)\n",
        "    \n",
        "    # Filter for error analysis\n",
        "    if config['analysis_mode'] == 'lost_tp':\n",
        "        analysis_df = df[df['error_type'] == 'Lost TP'].copy()\n",
        "        print(f\"\\nüî¥ Analyzing {len(analysis_df)} Lost True Positives\")\n",
        "    elif config['analysis_mode'] == 'kept_fp':\n",
        "        analysis_df = df[df['error_type'] == 'Kept FP'].copy()\n",
        "        print(f\"\\nüü° Analyzing {len(analysis_df)} Kept False Positives\")\n",
        "    else:\n",
        "        analysis_df = df[df['error_type'].isin(['Lost TP', 'Kept FP'])].copy()\n",
        "        print(f\"\\nüîç Analyzing {len(analysis_df)} total errors ({(df['error_type'] == 'Lost TP').sum()} Lost TP + {(df['error_type'] == 'Kept FP').sum()} Kept FP)\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No data available for analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Detailed Error Examples Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_error_example(row, include_prompt=False):\n",
        "    \"\"\"Display a detailed view of an error example.\"\"\"\n",
        "    \n",
        "    # Header with error type and metadata\n",
        "    error_color = \"üî¥\" if row['error_type'] == 'Lost TP' else \"üü°\"\n",
        "    \n",
        "    display(HTML(f\"\"\"\n",
        "    <div style=\"border: 2px solid {'#ff4444' if row['error_type'] == 'Lost TP' else '#ffaa00'}; \n",
        "                padding: 15px; margin: 10px 0; border-radius: 8px;\">\n",
        "        <h3>{error_color} {row['error_type']}: {row['smell_category']}</h3>\n",
        "        <p><strong>IaC Tool:</strong> {row['iac_tool']} | \n",
        "           <strong>File:</strong> {row['file_path']} | \n",
        "           <strong>Line:</strong> {row['line_number']}</p>\n",
        "        <p><strong>Ground Truth:</strong> {'‚úÖ True Positive' if row['is_true_positive'] else '‚ùå False Positive'} | \n",
        "           <strong>LLM Decision:</strong> {row['llm_decision']} | \n",
        "           <strong>Kept:</strong> {'‚úÖ Yes' if row['keep_detection'] else '‚ùå No'}</p>\n",
        "    </div>\n",
        "    \"\"\"))\n",
        "    \n",
        "    # Code context\n",
        "    print(\"üìã Code Context:\")\n",
        "    print(\"‚îÄ\" * 80)\n",
        "    print(row['context_snippet'])\n",
        "    print(\"‚îÄ\" * 80)\n",
        "    \n",
        "    # LLM response\n",
        "    if pd.notna(row['llm_raw_response']):\n",
        "        print(f\"\\nü§ñ LLM Raw Response: '{row['llm_raw_response']}'\")\n",
        "    \n",
        "    # Analysis notes\n",
        "    print(f\"\\nüìä Analysis Notes:\")\n",
        "    context_lines = len([l for l in str(row['context_snippet']).split('\\n') if l.strip() and not l.startswith('#')])\n",
        "    print(f\"   ‚Ä¢ Context Lines: {context_lines}\")\n",
        "    print(f\"   ‚Ä¢ Target Line Length: {len(str(row['target_content']))} chars\")\n",
        "    print(f\"   ‚Ä¢ Context Success: {row['context_success']}\")\n",
        "    print(f\"   ‚Ä¢ Processing Time: {row.get('llm_processing_time', 'N/A')}s\")\n",
        "    \n",
        "    return \"\\n\" + \"=\"*100 + \"\\n\"\n",
        "\n",
        "# Display examples based on configuration\n",
        "if not df.empty and 'analysis_df' in locals() and not analysis_df.empty:\n",
        "    # Better sampling strategy based on max_examples\n",
        "    if config['max_examples'] <= 10:\n",
        "        # For small numbers, prioritize variety (max 2 per group)\n",
        "        sample_df = analysis_df.groupby(['error_type', 'smell_category']).head(2).head(config['max_examples'])\n",
        "    else:\n",
        "        # For larger numbers, show more examples proportionally\n",
        "        examples_per_group = max(2, config['max_examples'] // len(analysis_df.groupby(['error_type', 'smell_category'])))\n",
        "        sample_df = analysis_df.groupby(['error_type', 'smell_category']).head(examples_per_group).head(config['max_examples'])\n",
        "    \n",
        "    print(f\"üîç Showing {len(sample_df)} out of {len(analysis_df)} total error examples:\")\n",
        "    print(f\"   (Max configured: {config['max_examples']}, Available: {len(analysis_df)})\")\n",
        "    \n",
        "    for i, (idx, row) in enumerate(sample_df.iterrows(), 1):\n",
        "        print(f\"\\n{'='*20} Example {i}/{len(sample_df)} {'='*20}\")\n",
        "        display_error_example(row)\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No examples to display\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° Analysis Summary & Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'analysis_df' in locals() and not analysis_df.empty:\n",
        "    print(\"üìã Error Analysis Summary:\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Summary statistics\n",
        "    total_errors = len(analysis_df)\n",
        "    lost_tp_count = (analysis_df['error_type'] == 'Lost TP').sum()\n",
        "    kept_fp_count = (analysis_df['error_type'] == 'Kept FP').sum()\n",
        "    \n",
        "    print(f\"üìä Total Errors Analyzed: {total_errors}\")\n",
        "    print(f\"   ‚Ä¢ Lost True Positives: {lost_tp_count} ({lost_tp_count/total_errors:.1%})\")\n",
        "    print(f\"   ‚Ä¢ Kept False Positives: {kept_fp_count} ({kept_fp_count/total_errors:.1%})\")\n",
        "    \n",
        "    # Context quality analysis\n",
        "    analysis_df['context_lines'] = analysis_df['context_snippet'].apply(\n",
        "        lambda x: len([line for line in str(x).split('\\n') if line.strip() and not line.startswith('#')])\n",
        "    )\n",
        "    avg_context_lines = analysis_df['context_lines'].mean()\n",
        "    context_success_rate = analysis_df['context_success'].mean()\n",
        "    \n",
        "    print(f\"\\nüìè Context Analysis:\")\n",
        "    print(f\"   ‚Ä¢ Average Context Lines: {avg_context_lines:.1f}\")\n",
        "    print(f\"   ‚Ä¢ Context Success Rate: {context_success_rate:.1%}\")\n",
        "    \n",
        "    # Smell-specific analysis\n",
        "    print(f\"\\nüîç Most Problematic Smells:\")\n",
        "    smell_error_rates = analysis_df.groupby('smell_category').size().sort_values(ascending=False)\n",
        "    for smell, count in smell_error_rates.head(3).items():\n",
        "        print(f\"   ‚Ä¢ {smell}: {count} errors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì§ Export Analysis Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'analysis_df' in locals() and not analysis_df.empty:\n",
        "    # Export detailed analysis to the experiment directory\n",
        "    export_dir = experiment_path / \"error_analysis\"\n",
        "    export_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Generate timestamp for this analysis\n",
        "    from datetime import datetime\n",
        "    analysis_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    print(f\"üìÅ Exporting analysis results to: {export_dir}\")\n",
        "    \n",
        "    # 1. Export filtered error examples\n",
        "    error_export_file = export_dir / f\"error_examples_{config['analysis_mode']}_{analysis_timestamp}.csv\"\n",
        "    \n",
        "    # Select key columns for export\n",
        "    export_columns = [\n",
        "        'detection_id', 'iac_tool', 'smell_category', 'file_path', 'line_number',\n",
        "        'is_true_positive', 'llm_decision', 'keep_detection', 'error_type',\n",
        "        'target_content', 'context_snippet', 'llm_raw_response', 'context_success',\n",
        "        'llm_processing_time', 'experiment_folder'\n",
        "    ]\n",
        "    \n",
        "    analysis_df[export_columns].to_csv(error_export_file, index=False)\n",
        "    print(f\"‚úÖ Exported {len(analysis_df)} error examples to: {error_export_file.name}\")\n",
        "    \n",
        "    # 2. Export summary statistics\n",
        "    summary_file = export_dir / f\"error_summary_{config['analysis_mode']}_{analysis_timestamp}.json\"\n",
        "    \n",
        "    # Calculate detailed statistics\n",
        "    analysis_df['context_lines'] = analysis_df['context_snippet'].apply(\n",
        "        lambda x: len([line for line in str(x).split('\\\\n') if line.strip() and not line.startswith('#')])\n",
        "    )\n",
        "    \n",
        "    summary_data = {\n",
        "        'analysis_metadata': {\n",
        "            'experiment_folder': config['experiment_folder'],\n",
        "            'analysis_timestamp': analysis_timestamp,\n",
        "            'analysis_config': config\n",
        "        },\n",
        "        'error_statistics': {\n",
        "            'total_errors': len(analysis_df),\n",
        "            'lost_tp_count': int((analysis_df['error_type'] == 'Lost TP').sum()),\n",
        "            'kept_fp_count': int((analysis_df['error_type'] == 'Kept FP').sum()),\n",
        "            'avg_context_lines': float(analysis_df['context_lines'].mean()),\n",
        "            'context_success_rate': float(analysis_df['context_success'].mean()),\n",
        "            'avg_processing_time': float(analysis_df['llm_processing_time'].mean()) if 'llm_processing_time' in analysis_df.columns else None\n",
        "        },\n",
        "        'distribution_analysis': {\n",
        "            'smell_error_distribution': analysis_df['smell_category'].value_counts().to_dict(),\n",
        "            'iac_tool_distribution': analysis_df['iac_tool'].value_counts().to_dict(),\n",
        "            'error_type_distribution': analysis_df['error_type'].value_counts().to_dict()\n",
        "        },\n",
        "        'context_analysis': {\n",
        "            'context_lines_stats': {\n",
        "                'min': int(analysis_df['context_lines'].min()),\n",
        "                'max': int(analysis_df['context_lines'].max()),\n",
        "                'mean': float(analysis_df['context_lines'].mean()),\n",
        "                'median': float(analysis_df['context_lines'].median())\n",
        "            },\n",
        "            'context_success_by_smell': analysis_df.groupby('smell_category')['context_success'].mean().to_dict()\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(summary_data, f, indent=2, default=str)\n",
        "    \n",
        "    print(f\"‚úÖ Exported analysis summary to: {summary_file.name}\")\n",
        "    \n",
        "    # 3. Export detailed examples for manual review (if not too many)\n",
        "    if len(analysis_df) <= 20:  # Only for manageable amounts\n",
        "        detailed_export_file = export_dir / f\"detailed_examples_{config['analysis_mode']}_{analysis_timestamp}.txt\"\n",
        "        \n",
        "        with open(detailed_export_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"DETAILED ERROR ANALYSIS REPORT\\\\n\")\n",
        "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
        "            f.write(f\"Experiment: {config['experiment_folder']}\\\\n\")\n",
        "            f.write(f\"Analysis Mode: {config['analysis_mode']}\\\\n\")\n",
        "            f.write(f\"Total Errors: {len(analysis_df)}\\\\n\")\n",
        "            f.write(\"=\"*80 + \"\\\\n\\\\n\")\n",
        "            \n",
        "            for i, (idx, row) in enumerate(analysis_df.iterrows(), 1):\n",
        "                f.write(f\"EXAMPLE {i}/{len(analysis_df)}\\\\n\")\n",
        "                f.write(f\"Error Type: {row['error_type']}\\\\n\")\n",
        "                f.write(f\"Smell: {row['smell_category']}\\\\n\")\n",
        "                f.write(f\"IaC Tool: {row['iac_tool']}\\\\n\")\n",
        "                f.write(f\"File: {row['file_path']}:{row['line_number']}\\\\n\")\n",
        "                f.write(f\"Ground Truth: {'TP' if row['is_true_positive'] else 'FP'}\\\\n\")\n",
        "                f.write(f\"LLM Decision: {row['llm_decision']}\\\\n\")\n",
        "                f.write(f\"Kept: {'Yes' if row['keep_detection'] else 'No'}\\\\n\")\n",
        "                f.write(\"\\\\nCode Context:\\\\n\")\n",
        "                f.write(\"-\" * 40 + \"\\\\n\")\n",
        "                f.write(str(row['context_snippet']) + \"\\\\n\")\n",
        "                f.write(\"-\" * 40 + \"\\\\n\")\n",
        "                f.write(f\"\\\\nLLM Response: {row['llm_raw_response']}\\\\n\")\n",
        "                f.write(\"\\\\n\" + \"=\"*80 + \"\\\\n\\\\n\")\n",
        "        \n",
        "        print(f\"‚úÖ Exported detailed examples to: {detailed_export_file.name}\")\n",
        "    \n",
        "    # 4. Create a README for the analysis\n",
        "    readme_file = export_dir / \"README.md\"\n",
        "    \n",
        "    readme_content = f'''# Error Analysis Results\n",
        "\n",
        "**Experiment:** {config['experiment_folder']}  \n",
        "**Analysis Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
        "**Analysis Mode:** {config['analysis_mode']}\n",
        "\n",
        "## Summary\n",
        "\n",
        "- **Total Errors Analyzed:** {len(analysis_df)}\n",
        "- **Lost True Positives:** {(analysis_df['error_type'] == 'Lost TP').sum()}\n",
        "- **Kept False Positives:** {(analysis_df['error_type'] == 'Kept FP').sum()}\n",
        "- **Average Context Lines:** {analysis_df['context_lines'].mean():.1f}\n",
        "- **Context Success Rate:** {analysis_df['context_success'].mean():.1%}\n",
        "\n",
        "## Files Generated\n",
        "\n",
        "1. `error_examples_{config['analysis_mode']}_{analysis_timestamp}.csv` - Detailed error data for further analysis\n",
        "2. `error_summary_{config['analysis_mode']}_{analysis_timestamp}.json` - Statistical summary and metadata\n",
        "3. `detailed_examples_{config['analysis_mode']}_{analysis_timestamp}.txt` - Human-readable detailed examples (if ‚â§20 errors)\n",
        "4. `README.md` - This file\n",
        "\n",
        "## Error Distribution\n",
        "\n",
        "### By Security Smell\n",
        "{chr(10).join([f\"- **{smell}:** {count} errors\" for smell, count in analysis_df['smell_category'].value_counts().items()])}\n",
        "\n",
        "### By IaC Tool  \n",
        "{chr(10).join([f\"- **{tool}:** {count} errors\" for tool, count in analysis_df['iac_tool'].value_counts().items()])}\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. Review detailed examples to identify patterns\n",
        "2. Consider adjusting context window size (current avg: {analysis_df['context_lines'].mean():.1f} lines)\n",
        "3. Refine prompt templates based on error patterns\n",
        "4. Test confidence-based filtering approaches\n",
        "\n",
        "---\n",
        "*Generated by 03_error_analysis.ipynb*\n",
        "'''\n",
        "    \n",
        "    with open(readme_file, 'w') as f:\n",
        "        f.write(readme_content)\n",
        "    \n",
        "    print(f\"‚úÖ Created analysis README: {readme_file.name}\")\n",
        "    \n",
        "    print(f\"\\\\nüéâ Analysis export completed!\")\n",
        "    print(f\"üìÇ All files saved to: {export_dir}\")\n",
        "    print(f\"\\\\nüìã Summary:\")\n",
        "    print(f\"   ‚Ä¢ Error examples CSV: {len(analysis_df)} rows\")\n",
        "    print(f\"   ‚Ä¢ Summary JSON with detailed statistics\")\n",
        "    print(f\"   ‚Ä¢ {'Detailed examples TXT (‚â§20 errors)' if len(analysis_df) <= 20 else 'Detailed examples skipped (>20 errors)'}\") \n",
        "    print(f\"   ‚Ä¢ README with human-readable summary\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No analysis data to export\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "research",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
