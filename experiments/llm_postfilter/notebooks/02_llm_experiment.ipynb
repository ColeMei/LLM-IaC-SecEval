{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8c73d4",
   "metadata": {},
   "source": [
    "# ğŸ¤– LLM Post-Filter Experiment: GLITCH+LLM Pipeline\n",
    "\n",
    "**Focus**: Evaluate **GLITCH + LLM** hybrid approach vs **GLITCH-only** baseline using **interactive configuration**.\n",
    "\n",
    "## ğŸ”¬ Experiment Pipeline:\n",
    "\n",
    "1. **Interactive Setup**: Configure LLM provider, model, and experiment parameters\n",
    "2. **Data Preparation**: GLITCH detections + context extracted *(01_data_extraction.py)*\n",
    "3. **LLM Filtering**: Apply the selected LLM post-filtering  \n",
    "4. **Performance Evaluation**: Calculate precision/recall improvements\n",
    "\n",
    "## ğŸ¯ Expected Outcomes:\n",
    "- **Precision**: 50-300% improvement\n",
    "- **Recall**: >90% retention  \n",
    "- **FP Reduction**: Significant decrease in false alarms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431bd6e",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ Interactive Experiment Configuration\n",
    "\n",
    "Configure your LLM provider, model, and experiment parameters using the interactive widgets below. No more manual cell editing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db9a05",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ğŸ›ï¸ Interactive Configuration\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "# Configuration storage\n",
    "config = {\n",
    "    'provider': 'openai',\n",
    "    'model': 'gpt-4o-mini', \n",
    "    'base_url': '',\n",
    "    'prompt_template': 'definition_based_conservative',\n",
    "    'context_lines': 3,\n",
    "    'iac_tools': ['chef', 'puppet']\n",
    "}\n",
    "\n",
    "# Provider selection\n",
    "provider_widget = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('ğŸ”¥ OpenAI (GPT models)', 'openai'),\n",
    "        ('ğŸ§  Anthropic (Claude models)', 'anthropic'), \n",
    "        ('ğŸ¦™ Ollama (Local models)', 'ollama')\n",
    "    ],\n",
    "    value=config['provider'],\n",
    "    description='Provider:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Model input\n",
    "model_widget = widgets.Text(\n",
    "    value=config['model'],\n",
    "    placeholder='e.g., gpt-4o, claude-3-5-sonnet-latest, codellama:7b',\n",
    "    description='Model:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Base URL input (for ollama)\n",
    "base_url_widget = widgets.Text(\n",
    "    value=config['base_url'],\n",
    "    placeholder='e.g., http://localhost:11434',\n",
    "    description='Base URL:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Prompt template selection (dynamic detection)\n",
    "prompt_template_widget = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Definition Based Conservative', 'definition_based_conservative'),\n",
    "        ('Definition Based Current', 'definition_based_current'),\n",
    "        ('Static Analysis Rules Conservative', 'static_analysis_rules_conservative'),\n",
    "        ('Static Analysis Rules Current', 'static_analysis_rules_current'),\n",
    "    ],\n",
    "    value=config['prompt_template'],\n",
    "    description='Prompt Template:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Context lines slider\n",
    "context_lines_widget = widgets.IntSlider(\n",
    "    value=config['context_lines'],\n",
    "    min=0,\n",
    "    max=3,\n",
    "    step=1,\n",
    "    description='Context Lines:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# IaC tools selection\n",
    "iac_tools_widget = widgets.ToggleButtons(\n",
    "    options=[\n",
    "        ('Chef', 'chef'),\n",
    "        ('Puppet', 'puppet'),\n",
    "        ('Both', 'both')\n",
    "    ],\n",
    "    value='both',\n",
    "    description='IaC Tools:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Status output\n",
    "status_output = widgets.Output()\n",
    "\n",
    "def update_config(*args):\n",
    "    \"\"\"Update configuration and environment variables when widgets change\"\"\"\n",
    "    with status_output:\n",
    "        clear_output()\n",
    "        \n",
    "        # Update config\n",
    "        config['provider'] = provider_widget.value\n",
    "        config['model'] = model_widget.value\n",
    "        config['base_url'] = base_url_widget.value.strip()\n",
    "        config['prompt_template'] = prompt_template_widget.value\n",
    "        config['context_lines'] = context_lines_widget.value\n",
    "        \n",
    "        # Handle IaC tools selection\n",
    "        if iac_tools_widget.value == 'both':\n",
    "            config['iac_tools'] = ['chef', 'puppet']\n",
    "        elif iac_tools_widget.value == 'chef':\n",
    "            config['iac_tools'] = ['chef']\n",
    "        elif iac_tools_widget.value == 'puppet':\n",
    "            config['iac_tools'] = ['puppet']\n",
    "        else:\n",
    "            config['iac_tools'] = []\n",
    "        \n",
    "        # Update environment variables\n",
    "        os.environ[\"LLM_PROVIDER\"] = config['provider']\n",
    "        os.environ[\"LLM_MODEL\"] = config['model']\n",
    "        if config['base_url']:\n",
    "            os.environ[\"LLM_BASE_URL\"] = config['base_url']\n",
    "        else:\n",
    "            os.environ.pop(\"LLM_BASE_URL\", None)\n",
    "        \n",
    "        # Show configuration status\n",
    "        provider_emoji = {\n",
    "            'openai': 'ğŸ”¥', 'anthropic': 'ğŸ§ ', \n",
    "            'ollama': 'ğŸ¦™'\n",
    "        }\n",
    "        \n",
    "        print(\"âœ… CONFIGURATION UPDATED\")\n",
    "        print(f\"ğŸ¤– Provider: {provider_emoji.get(config['provider'], 'â“')} {config['provider']}\")\n",
    "        print(f\"ğŸ¯ Model: {config['model']}\")\n",
    "        if config['base_url']:\n",
    "            print(f\"ğŸŒ Base URL: {config['base_url']}\")\n",
    "        print(f\"ğŸ“‹ Prompt Template: {config['prompt_template']}\")\n",
    "        print(f\"ğŸ” Context Lines: Â±{config['context_lines']}\")\n",
    "        print(f\"ğŸ”§ IaC Tools: {', '.join(config['iac_tools']) if config['iac_tools'] else 'None selected'}\")\n",
    "        \n",
    "        # API key check\n",
    "        api_key_status = \"âŒ Not found\"\n",
    "        if config['provider'] == 'openai' and os.getenv(\"OPENAI_API_KEY\"):\n",
    "            api_key_status = \"âœ… Found\"\n",
    "        elif config['provider'] == 'anthropic' and os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "            api_key_status = \"âœ… Found\"\n",
    "        elif config['provider'] == 'ollama':\n",
    "            api_key_status = \"ğŸš« Not required\"\n",
    "            \n",
    "        print(f\"ğŸ”‘ API Key: {api_key_status}\")\n",
    "\n",
    "# Display widgets\n",
    "display(provider_widget)\n",
    "display(model_widget)\n",
    "display(base_url_widget)\n",
    "display(prompt_template_widget)\n",
    "display(context_lines_widget)\n",
    "display(iac_tools_widget)\n",
    "display(status_output)\n",
    "\n",
    "# Initial configuration display\n",
    "update_config()\n",
    "\n",
    "# Connect widgets to update function (after initial display to avoid duplicates)\n",
    "provider_widget.observe(update_config, names='value')\n",
    "model_widget.observe(update_config, names='value')\n",
    "base_url_widget.observe(update_config, names='value')\n",
    "prompt_template_widget.observe(update_config, names='value')\n",
    "context_lines_widget.observe(update_config, names='value')\n",
    "iac_tools_widget.observe(update_config, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbe2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add llm-postfilter modules to path\n",
    "project_root = Path.cwd().parent.parent.parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "# Import llm-postfilter pipeline components\n",
    "from llm_postfilter import (\n",
    "    GLITCHLLMFilter, \n",
    "    HybridEvaluator,\n",
    "    SecuritySmellPrompts,\n",
    "    SecuritySmell,\n",
    "    Provider,\n",
    ")\n",
    "\n",
    "print(f\"ğŸ  Project root: {project_root}\")\n",
    "print(f\"ğŸ“ Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Get configuration from interactive widgets\n",
    "provider = config['provider']\n",
    "model = config['model']\n",
    "base_url = config['base_url'] if config['base_url'] else None\n",
    "prompt_template = config['prompt_template']\n",
    "context_lines = config['context_lines']\n",
    "iac_tools = config['iac_tools']\n",
    "\n",
    "# API keys per provider\n",
    "api_key = None\n",
    "if provider == Provider.OPENAI.value:\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print(f\"ğŸ”‘ Provider: OpenAI | Model: {model} | API key found: {bool(api_key)}\")\n",
    "elif provider == Provider.ANTHROPIC.value:\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    print(f\"ğŸ”‘ Provider: Anthropic | Model: {model} | API key found: {bool(api_key)}\")\n",
    "elif provider == Provider.OLLAMA.value:\n",
    "    print(f\"ğŸ”‘ Provider: Ollama | Model: {model} | Base URL: {base_url or 'http://localhost:11434'}\")\n",
    "else:\n",
    "    print(f\"âŒ Unsupported provider: {provider}\")\n",
    "\n",
    "print(\"\\nğŸ“Š CURRENT EXPERIMENT CONFIGURATION:\")\n",
    "print(f\"  ğŸ¤– Provider: {provider}\")\n",
    "print(f\"  ğŸ¯ Model: {model}\")\n",
    "if base_url:\n",
    "    print(f\"  ğŸŒ Base URL: {base_url}\")\n",
    "print(f\"  ğŸ“‹ Prompt Template: {prompt_template}\")\n",
    "print(f\"  ğŸ” Context Lines: Â±{context_lines}\")\n",
    "print(f\"  ğŸ”§ IaC Tools: {', '.join(iac_tools) if iac_tools else 'None selected'}\")\n",
    "\n",
    "print(\"\\nğŸš€ LLM Post-Filter Experiment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa37ea",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 1: Load Context-Enhanced Data\n",
    "\n",
    "**Load detection files with code context prepared by 01_data_extraction.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "data_dir = project_root / \"experiments/llm_postfilter/data\"\n",
    "context_dir = data_dir / \"with_context\"\n",
    "\n",
    "# Find context-enhanced files\n",
    "all_context_files = list(context_dir.glob(\"*_with_context.csv\"))\n",
    "\n",
    "# Filter files based on selected IaC tools\n",
    "context_enhanced_files = []\n",
    "for file in all_context_files:\n",
    "    for tool in iac_tools:\n",
    "        if file.name.startswith(tool):\n",
    "            context_enhanced_files.append(file)\n",
    "            break\n",
    "\n",
    "print(f\"ğŸ“ Available files: {len(all_context_files)} | Selected tools: {', '.join(iac_tools) if iac_tools else 'None'}\")\n",
    "\n",
    "if context_enhanced_files:\n",
    "    print(f\"ğŸ“ Processing {len(context_enhanced_files)} context-enhanced files:\")\n",
    "    for file in context_enhanced_files:\n",
    "        df = pd.read_csv(file)\n",
    "        tp_count = df['is_true_positive'].sum()\n",
    "        fp_count = len(df) - tp_count\n",
    "        context_success = df['context_success'].sum()\n",
    "        print(f\"  ğŸ“„ {file.name}: {len(df)} detections ({tp_count} TP, {fp_count} FP, {context_success} with context)\")\n",
    "    \n",
    "    print(f\"\\nâœ… Context-enhanced data ready for LLM analysis\")\n",
    "    \n",
    "elif not iac_tools:\n",
    "    print(\"âŒ No IaC tools selected!\")\n",
    "    print(\"â¡ï¸  Select Chef and/or Puppet in the configuration above\")\n",
    "else:\n",
    "    print(\"âŒ No context-enhanced files found for selected tools!\")\n",
    "    print(\"â¡ï¸  Run 01_data_extraction.py first to prepare the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c84b9",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 2: Review LLM Prompt Design\n",
    "\n",
    "Review the formal security smell definitions used for LLM evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29788a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Prompt Template Demonstration  \n",
    "print(\"ğŸ” PROMPT TEMPLATE PREVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample code for demonstration\n",
    "sample_code = \"\"\"\n",
    "# File: database_config.rb\n",
    "    15: database_password = \"MySecretPassword123\"\n",
    ">>> 16: connection_string = \"mongodb://admin:#{database_password}@localhost:27017/mydb\"\n",
    "    17: Chef::Log.info(\"Connecting to database\")\n",
    "\"\"\"\n",
    "\n",
    "smell = SecuritySmell.HARD_CODED_SECRET\n",
    "current_template = config['prompt_template']\n",
    "\n",
    "print(f\"\\nğŸ“ CURRENT TEMPLATE: {current_template}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "prompt = SecuritySmellPrompts.create_prompt(smell, sample_code, current_template)\n",
    "\n",
    "# Show first few lines to preview\n",
    "lines = prompt.split('\\n')[:8]\n",
    "for line in lines:\n",
    "    print(f\"  {line}\")\n",
    "\n",
    "print(f\"  ... ({len(prompt)} total characters)\")\n",
    "\n",
    "# Show available templates\n",
    "print(f\"\\nğŸ“‹ Available Templates:\")\n",
    "for template in SecuritySmellPrompts.get_available_templates():\n",
    "    marker = \"ğŸŸ¢ SELECTED\" if template == current_template else \"ğŸ”µ Available\"\n",
    "    print(f\"  {marker} - {template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042b49d",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 3: Initialize LLM Pipeline\n",
    "\n",
    "Setup GLITCH+LLM hybrid detection pipeline with the selected provider/model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a897c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Initialize the LLM filter pipeline\n",
    "# For OpenAI/Anthropic/OpenAI-compatible, an API key is required; for Ollama, it's not\n",
    "if (provider == Provider.OLLAMA.value) or (api_key):\n",
    "    print(\"ğŸ”§ Initializing GLITCH+LLM pipeline...\")\n",
    "    \n",
    "    llm_filter = GLITCHLLMFilter(\n",
    "        project_root=project_root,\n",
    "        api_key=api_key,\n",
    "        model=model,\n",
    "        provider=provider,\n",
    "        base_url=base_url,\n",
    "        context_lines=context_lines,\n",
    "        prompt_template=prompt_template,\n",
    "    )\n",
    "    evaluator = HybridEvaluator(project_root)\n",
    "    \n",
    "    # Setup directories\n",
    "    results_dir = data_dir / \"llm_results\"\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"âœ… Pipeline ready:\")\n",
    "    print(f\"  ğŸ¤– Provider: {provider}\")\n",
    "    print(f\"  ğŸ¯ Model: {llm_filter.llm_client.model}\")\n",
    "    print(f\"  ğŸ” Context lines: Â±{llm_filter.context_lines}\")\n",
    "    print(f\"  ğŸ“‹ Prompt template: {llm_filter.prompt_template}\")\n",
    "    print(f\"  ğŸ“Š Results â†’ {results_dir}\")\n",
    "    \n",
    "    # Configuration validation\n",
    "    print(f\"\\nğŸ” Pipeline Configuration Validation:\")\n",
    "    print(f\"  âœ… Using interactive config - provider: {config['provider']}\")\n",
    "    print(f\"  âœ… Using interactive config - model: {config['model']}\")\n",
    "    print(f\"  âœ… Using interactive config - prompt_template: {config['prompt_template']}\")\n",
    "    print(f\"  âœ… Using interactive config - context_lines: {config['context_lines']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Pipeline initialization failed - missing credentials\")\n",
    "    print(\"ğŸ’¡ Solutions:\")\n",
    "    print(\"  â€¢ Set API key environment variable:\")\n",
    "    if provider == Provider.OPENAI.value:\n",
    "        print(\"    export OPENAI_API_KEY='your-key-here'\")\n",
    "    elif provider == Provider.ANTHROPIC.value:\n",
    "        print(\"    export ANTHROPIC_API_KEY='your-key-here'\")\n",
    "    print(\"  â€¢ Or switch to Ollama (no API key required)\")\n",
    "    print(\"  â€¢ Update configuration using the widgets above â¬†ï¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3adbf",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 4: Run LLM Post-Filtering\n",
    "\n",
    "Apply LLM post-filtering to GLITCH detections and measure improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6901bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((provider == Provider.OLLAMA.value) or api_key) and 'context_enhanced_files' in locals():\n",
    "    print(f\"ğŸ” Processing {len(context_enhanced_files)} context-enhanced files:\")\n",
    "    for file in context_enhanced_files:\n",
    "        df = pd.read_csv(file)\n",
    "        tp_count = df['is_true_positive'].sum()\n",
    "        fp_count = len(df) - tp_count\n",
    "        context_success = df['context_success'].sum()\n",
    "        print(f\"  ğŸ“ {file.name}: {len(df)} detections ({tp_count} TP, {fp_count} FP)\")\n",
    "    \n",
    "    print(f\"\\nğŸš€ Starting LLM post-filtering...\")\n",
    "    \n",
    "    # Process each context-enhanced file\n",
    "    filtered_results = {}\n",
    "    \n",
    "    for i, context_file in enumerate(context_enhanced_files):\n",
    "        print(f\"\\nğŸ”„ Processing {i+1}/{len(context_enhanced_files)}: {context_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Run LLM filtering\n",
    "            filtered_df = llm_filter.filter_detections(context_file, results_dir)\n",
    "            filtered_results[context_file.stem] = filtered_df\n",
    "            \n",
    "            # Summary stats\n",
    "            total = len(filtered_df)\n",
    "            kept = filtered_df['keep_detection'].sum()\n",
    "            original_tp = filtered_df['is_true_positive'].sum() \n",
    "            kept_tp = filtered_df[filtered_df['keep_detection']]['is_true_positive'].sum()\n",
    "            \n",
    "            print(f\"âœ… Kept {kept}/{total} ({kept/total:.1%}) | TP retention: {kept_tp}/{original_tp} ({kept_tp/original_tp:.1%})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "            logger.error(f\"Failed to process {context_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ LLM filtering completed! Results â†’ {results_dir}\")\n",
    "    \n",
    "elif provider != Provider.OLLAMA.value and not api_key:\n",
    "    print(\"âŒ Skipping - API key required for selected provider\")\n",
    "else:\n",
    "    print(\"âŒ Skipping - no context files (run context extraction first)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c968693",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 5: Evaluate Performance Improvement\n",
    "\n",
    "Calculate precision, recall, and F1 improvements from LLM post-filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'filtered_results' in locals() and filtered_results:\n",
    "    print(\"ğŸ“Š Evaluating GLITCH vs GLITCH+LLM Performance\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Organize results by IaC tool (only process selected tools)\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for tool in iac_tools:\n",
    "        tool_filtered_dfs = []\n",
    "        for key, filtered_df in filtered_results.items():\n",
    "            if key.startswith(tool):\n",
    "                tool_filtered_dfs.append(filtered_df)\n",
    "        \n",
    "        if tool_filtered_dfs:\n",
    "            tool_results = evaluator.evaluate_iac_tool(tool_filtered_dfs, tool.title())\n",
    "            evaluation_results[tool] = tool_results\n",
    "    \n",
    "    # Save evaluation results\n",
    "    evaluation_dir = results_dir / \"evaluation\"\n",
    "    evaluation_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    if evaluation_results:\n",
    "        summary_df = evaluator.save_evaluation_results(evaluation_results, evaluation_dir)\n",
    "        \n",
    "        print(\"\\nğŸ¯ EXPERIMENT RESULTS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Display key findings\n",
    "        for _, row in summary_df.iterrows():\n",
    "            tool = row['IaC_Tool']\n",
    "            smell = row['Security_Smell']\n",
    "            baseline_precision = row['Baseline_Precision']\n",
    "            llm_precision = row['LLM_Precision']\n",
    "            precision_improvement = row['Precision_Improvement']\n",
    "            fp_reduction = row['FP_Reduction']\n",
    "            tp_retention = row['TP_Retention']\n",
    "            \n",
    "            print(f\"\\nğŸ“Œ {tool} - {smell}:\")\n",
    "            print(f\"  Precision: {baseline_precision:.3f} â†’ {llm_precision:.3f} ({precision_improvement:+.1%})\")\n",
    "            print(f\"  FPâ†“: {fp_reduction:.1%} | TP retained: {tp_retention:.1%}\")\n",
    "        \n",
    "        # Overall improvements\n",
    "        avg_precision_improvement = summary_df['Precision_Improvement'].mean()\n",
    "        avg_fp_reduction = summary_df['FP_Reduction'].mean()\n",
    "        avg_tp_retention = summary_df['TP_Retention'].mean()\n",
    "        \n",
    "        print(f\"\\nğŸš€ OVERALL OUTCOMES:\")\n",
    "        print(f\"  ğŸ“ˆ Precision improvement: {avg_precision_improvement:+.1%}\")\n",
    "        print(f\"  ğŸ“‰ FP reduction: {avg_fp_reduction:.1%}\")\n",
    "        print(f\"  ğŸ¯ TP retention: {avg_tp_retention:.1%}\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Detailed results â†’ {evaluation_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No evaluation results available\")\n",
    "        \n",
    "else:\n",
    "    print(\"â­ï¸ Skipping evaluation - run LLM filtering first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dfbd1e",
   "metadata": {},
   "source": [
    "## ğŸ“ Generated Files & Transparency\n",
    "\n",
    "Complete experimental transparency through intermediate files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae4f656",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "print(\"ğŸ“ Generated Files & Transparency\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nğŸ” Context Files (LLM input):\")\n",
    "if 'context_dir' in locals():\n",
    "    context_files = list(context_dir.glob(\"*.csv\"))\n",
    "    for file in context_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        print(f\"  ğŸ“„ {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  ğŸ“ {context_dir}\")\n",
    "else:\n",
    "    print(\"  âŒ No context files generated\")\n",
    "\n",
    "print(\"\\nğŸ¤– LLM Results:\")\n",
    "if 'results_dir' in locals() and results_dir.exists():\n",
    "    result_files = list(results_dir.glob(\"*.csv\")) + list(results_dir.glob(\"*.json\"))\n",
    "    for file in result_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        if file.name.endswith(\"_prompts_and_responses.json\"):\n",
    "            print(f\"  ğŸ“ {file.name} ({size_kb} KB) - Full prompts & LLM responses\")\n",
    "        elif file.name.endswith(\"_llm_filtered.csv\"):\n",
    "            print(f\"  ğŸ“Š {file.name} ({size_kb} KB) - Filtered detections\")\n",
    "        else:\n",
    "            print(f\"  ğŸ“„ {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  ğŸ“ {results_dir}\")\n",
    "else:\n",
    "    print(\"  âŒ No LLM results generated\")\n",
    "\n",
    "print(\"\\nğŸ“Š Evaluation:\")\n",
    "if 'evaluation_dir' in locals() and evaluation_dir.exists():\n",
    "    eval_files = list(evaluation_dir.glob(\"*.csv\")) + list(evaluation_dir.glob(\"*.json\"))\n",
    "    for file in eval_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        print(f\"  ğŸ“„ {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  ğŸ“ {evaluation_dir}\")\n",
    "else:\n",
    "    print(\"  âŒ No evaluation results generated\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Full transparency: code snippets, prompts, LLM decisions, metrics\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
