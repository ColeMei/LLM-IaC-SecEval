{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e32593d",
   "metadata": {},
   "source": [
    "# ü§ñ LLM Post-Filter Experiment: GLITCH+LLM Pipeline\n",
    "\n",
    "**Focus**: Evaluate **GLITCH + LLM** hybrid approach vs **GLITCH-only** baseline.\n",
    "\n",
    "## üî¨ Experiment Pipeline:\n",
    "\n",
    "1. **Data Preparation**: GLITCH detections + context extracted *(01_data_extraction.py)*\n",
    "2. **LLM Filtering**: Apply the selected LLM post-filtering  \n",
    "3. **Performance Evaluation**: Calculate precision/recall improvements\n",
    "\n",
    "## üéØ Expected Outcomes:\n",
    "- **Precision**: 50-300% improvement\n",
    "- **Recall**: >90% retention  \n",
    "- **FP Reduction**: Significant decrease in false alarms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52b23a",
   "metadata": {},
   "source": [
    "## üîß Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe85201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Manual provider/model override (optional)\n",
    "# Edit these values to force a specific provider/model for this session.\n",
    "# Supported providers: \"openai\", \"anthropic\", \"ollama\", \"openai_compatible\"\n",
    "provider = \"openai\"\n",
    "model = \"gpt-4o\"   # examples: \"gpt-4o\", \"claude-3-5-sonnet-latest\", \"codellama:7b\"\n",
    "base_url = None          # for ollama or openai-compatible, e.g. \"http://localhost:11434\" or \"https://api.x.ai/v1\"\n",
    "\n",
    "import os\n",
    "os.environ[\"LLM_PROVIDER\"] = provider\n",
    "os.environ[\"LLM_MODEL\"] = model\n",
    "if base_url:\n",
    "    os.environ[\"LLM_BASE_URL\"] = str(base_url)\n",
    "else:\n",
    "    os.environ.pop(\"LLM_BASE_URL\", None)\n",
    "\n",
    "print(f\"‚úÖ Using provider={os.getenv('LLM_PROVIDER')} | model={os.getenv('LLM_MODEL')} | base_url={os.getenv('LLM_BASE_URL')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîé Context window configuration (lines around target; use 0 for target-only)\n",
    "CONTEXT_LINES = 3  # change to 0, 1, 2, ... as needed\n",
    "print(f\"Context lines: ¬±{CONTEXT_LINES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add llm-postfilter modules to path\n",
    "project_root = Path.cwd().parent.parent.parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "# Import llm-postfilter pipeline components\n",
    "from llm_postfilter import (\n",
    "    GLITCHLLMFilter, \n",
    "    HybridEvaluator,\n",
    "    SecuritySmellPrompts,\n",
    "    SecuritySmell,\n",
    "    Provider,\n",
    ")\n",
    "\n",
    "print(f\"üè† Project root: {project_root}\")\n",
    "print(f\"üìç Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Optional: set provider/model for downstream notebook via env\n",
    "os.environ[\"LLM_PROVIDER\"] = \"openai\"\n",
    "os.environ[\"LLM_MODEL\"] = \"gpt-4o\"\n",
    "\n",
    "# Provider/model selection via env vars with sensible defaults\n",
    "provider = os.getenv(\"LLM_PROVIDER\", Provider.OPENAI.value)\n",
    "model = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
    "base_url = os.getenv(\"LLM_BASE_URL\")  # for ollama or openai-compatible\n",
    "\n",
    "# API keys per provider\n",
    "api_key = None\n",
    "if provider == Provider.OPENAI.value:\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print(f\"üîë Provider: OpenAI | Model: {model} | API key found: {bool(api_key)}\")\n",
    "elif provider == Provider.ANTHROPIC.value:\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    print(f\"üîë Provider: Anthropic | Model: {model} | API key found: {bool(api_key)}\")\n",
    "elif provider == Provider.OLLAMA.value:\n",
    "    print(f\"üîë Provider: Ollama | Model: {model} | Base URL: {base_url or 'http://localhost:11434'}\")\n",
    "elif provider == Provider.OPENAI_COMPATIBLE.value:\n",
    "    api_key = os.getenv(\"OPENAI_COMPATIBLE_API_KEY\")\n",
    "    print(f\"üîë Provider: OpenAI-compatible | Model: {model} | Base URL: {base_url}\")\n",
    "else:\n",
    "    print(f\"‚ùå Unsupported provider: {provider}\")\n",
    "\n",
    "print(\"üöÄ LLM Post-Filter Experiment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa31952a",
   "metadata": {},
   "source": [
    "## üìÅ Step 1: Load Context-Enhanced Data\n",
    "\n",
    "**Load detection files with code context prepared by 01_data_extraction.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "data_dir = project_root / \"experiments/llm_postfilter/data\"\n",
    "context_dir = data_dir / \"with_context\"\n",
    "\n",
    "# Find context-enhanced files\n",
    "context_enhanced_files = list(context_dir.glob(\"*_with_context.csv\"))\n",
    "\n",
    "if context_enhanced_files:\n",
    "    print(f\"üìÅ Found {len(context_enhanced_files)} context-enhanced files:\")\n",
    "    for file in context_enhanced_files:\n",
    "        df = pd.read_csv(file)\n",
    "        tp_count = df['is_true_positive'].sum()\n",
    "        fp_count = len(df) - tp_count\n",
    "        context_success = df['context_success'].sum()\n",
    "        print(f\"  üìÑ {file.name}: {len(df)} detections ({tp_count} TP, {fp_count} FP, {context_success} with context)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Context-enhanced data ready for LLM analysis\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No context-enhanced files found!\")\n",
    "    print(\"‚û°Ô∏è  Run 01_data_extraction.py first to prepare the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79241f",
   "metadata": {},
   "source": [
    "## üìù Step 2: Review LLM Prompt Design\n",
    "\n",
    "Review the formal security smell definitions used for LLM evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ee4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display formal definitions for each security smell\n",
    "print(\"üìù Security Smell Definitions for LLM\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for smell in SecuritySmell:\n",
    "    definition = SecuritySmellPrompts.DEFINITIONS[smell]\n",
    "    lines = definition.strip().split('\\n')[:3]\n",
    "    print(f\"\\nüìå {smell.value}\")\n",
    "    for line in lines:\n",
    "        print(f\"  {line}\")\n",
    "    print(f\"  ... ({len(definition.split())} words total)\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(SecuritySmell)} smell categories with formal definitions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf30b6d",
   "metadata": {},
   "source": [
    "## üîß Step 3: Initialize LLM Pipeline\n",
    "\n",
    "Setup GLITCH+LLM hybrid detection pipeline with the selected provider/model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM filter pipeline\n",
    "# For OpenAI/Anthropic/OpenAI-compatible, an API key is required; for Ollama, it's not\n",
    "if (provider == Provider.OLLAMA.value) or (api_key):\n",
    "    print(\"üîß Initializing GLITCH+LLM pipeline...\")\n",
    "    \n",
    "    llm_filter = GLITCHLLMFilter(\n",
    "        project_root=project_root,\n",
    "        api_key=api_key,\n",
    "        model=model,\n",
    "        provider=provider,\n",
    "        base_url=base_url,\n",
    "        context_lines=CONTEXT_LINES,\n",
    "    )\n",
    "    evaluator = HybridEvaluator(project_root)\n",
    "    \n",
    "    # Setup directories\n",
    "    results_dir = data_dir / \"llm_results\"\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"‚úÖ Pipeline ready:\")\n",
    "    print(f\"  ü§ñ Provider: {provider}\")\n",
    "    print(f\"  ü§ñ Model: {llm_filter.llm_client.model}\")\n",
    "    print(f\"  üß© Context lines: ¬±{llm_filter.context_lines}\")\n",
    "    print(f\"  üìä Results ‚Üí {results_dir}\")\n",
    "else:\n",
    "    print(\"‚ùå Pipeline initialization failed - missing credentials\")\n",
    "    print(\"Set appropriate API key env var (OPENAI_API_KEY / ANTHROPIC_API_KEY / OPENAI_COMPATIBLE_API_KEY) or use Ollama\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891df435",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Run LLM Post-Filtering\n",
    "\n",
    "Apply LLM post-filtering to GLITCH detections and measure improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b00380",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((provider == Provider.OLLAMA.value) or api_key) and 'context_enhanced_files' in locals():\n",
    "    print(f\"üîç Processing {len(context_enhanced_files)} context-enhanced files:\")\n",
    "    for file in context_enhanced_files:\n",
    "        df = pd.read_csv(file)\n",
    "        tp_count = df['is_true_positive'].sum()\n",
    "        fp_count = len(df) - tp_count\n",
    "        context_success = df['context_success'].sum()\n",
    "        print(f\"  üìÅ {file.name}: {len(df)} detections ({tp_count} TP, {fp_count} FP)\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting LLM post-filtering...\")\n",
    "    \n",
    "    # Process each context-enhanced file\n",
    "    filtered_results = {}\n",
    "    \n",
    "    for i, context_file in enumerate(context_enhanced_files):\n",
    "        print(f\"\\nüîÑ Processing {i+1}/{len(context_enhanced_files)}: {context_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Run LLM filtering\n",
    "            filtered_df = llm_filter.filter_detections(context_file, results_dir)\n",
    "            filtered_results[context_file.stem] = filtered_df\n",
    "            \n",
    "            # Summary stats\n",
    "            total = len(filtered_df)\n",
    "            kept = filtered_df['keep_detection'].sum()\n",
    "            original_tp = filtered_df['is_true_positive'].sum() \n",
    "            kept_tp = filtered_df[filtered_df['keep_detection']]['is_true_positive'].sum()\n",
    "            \n",
    "            print(f\"‚úÖ Kept {kept}/{total} ({kept/total:.1%}) | TP retention: {kept_tp}/{original_tp} ({kept_tp/original_tp:.1%})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            logger.error(f\"Failed to process {context_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ LLM filtering completed! Results ‚Üí {results_dir}\")\n",
    "    \n",
    "elif provider != Provider.OLLAMA.value and not api_key:\n",
    "    print(\"‚ùå Skipping - API key required for selected provider\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping - no context files (run context extraction first)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5c3aa",
   "metadata": {},
   "source": [
    "## üìà Step 5: Evaluate Performance Improvement\n",
    "\n",
    "Calculate precision, recall, and F1 improvements from LLM post-filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc577f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if api_key and 'filtered_results' in locals():\n",
    "    print(\"üìä Evaluating GLITCH vs GLITCH+LLM Performance\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Organize results by IaC tool\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for tool in ['chef', 'puppet']:\n",
    "        tool_filtered_dfs = []\n",
    "        for key, filtered_df in filtered_results.items():\n",
    "            if key.startswith(tool):\n",
    "                tool_filtered_dfs.append(filtered_df)\n",
    "        \n",
    "        if tool_filtered_dfs:\n",
    "            tool_results = evaluator.evaluate_iac_tool(tool_filtered_dfs, tool.title())\n",
    "            evaluation_results[tool] = tool_results\n",
    "    \n",
    "    # Save evaluation results\n",
    "    evaluation_dir = results_dir / \"evaluation\"\n",
    "    evaluation_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    if evaluation_results:\n",
    "        summary_df = evaluator.save_evaluation_results(evaluation_results, evaluation_dir)\n",
    "        \n",
    "        print(\"\\nüéØ EXPERIMENT RESULTS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Display key findings\n",
    "        for _, row in summary_df.iterrows():\n",
    "            tool = row['IaC_Tool']\n",
    "            smell = row['Security_Smell']\n",
    "            baseline_precision = row['Baseline_Precision']\n",
    "            llm_precision = row['LLM_Precision']\n",
    "            precision_improvement = row['Precision_Improvement']\n",
    "            fp_reduction = row['FP_Reduction']\n",
    "            tp_retention = row['TP_Retention']\n",
    "            \n",
    "            print(f\"\\nüìå {tool} - {smell}:\")\n",
    "            print(f\"  Precision: {baseline_precision:.3f} ‚Üí {llm_precision:.3f} ({precision_improvement:+.1%})\")\n",
    "            print(f\"  FP‚Üì: {fp_reduction:.1%} | TP retained: {tp_retention:.1%}\")\n",
    "        \n",
    "        # Overall improvements\n",
    "        avg_precision_improvement = summary_df['Precision_Improvement'].mean()\n",
    "        avg_fp_reduction = summary_df['FP_Reduction'].mean()\n",
    "        avg_tp_retention = summary_df['TP_Retention'].mean()\n",
    "        \n",
    "        print(f\"\\nüöÄ OVERALL OUTCOMES:\")\n",
    "        print(f\"  üìà Precision improvement: {avg_precision_improvement:+.1%}\")\n",
    "        print(f\"  üìâ FP reduction: {avg_fp_reduction:.1%}\")\n",
    "        print(f\"  üéØ TP retention: {avg_tp_retention:.1%}\")\n",
    "        \n",
    "        print(f\"\\nüíæ Detailed results ‚Üí {evaluation_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No evaluation results available\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping evaluation - run LLM filtering first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224034f6",
   "metadata": {},
   "source": [
    "## üìÅ Generated Files & Transparency\n",
    "\n",
    "Complete experimental transparency through intermediate files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ad639",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "print(\"üìÅ Generated Files & Transparency\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nüîç Context Files (LLM input):\")\n",
    "if 'context_dir' in locals():\n",
    "    context_files = list(context_dir.glob(\"*.csv\"))\n",
    "    for file in context_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        print(f\"  üìÑ {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  üìÅ {context_dir}\")\n",
    "else:\n",
    "    print(\"  ‚ùå No context files generated\")\n",
    "\n",
    "print(\"\\nü§ñ LLM Results:\")\n",
    "if 'results_dir' in locals() and results_dir.exists():\n",
    "    result_files = list(results_dir.glob(\"*.csv\")) + list(results_dir.glob(\"*.json\"))\n",
    "    for file in result_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        if file.name.endswith(\"_prompts_and_responses.json\"):\n",
    "            print(f\"  üìù {file.name} ({size_kb} KB) - Full prompts & LLM responses\")\n",
    "        elif file.name.endswith(\"_llm_filtered.csv\"):\n",
    "            print(f\"  üìä {file.name} ({size_kb} KB) - Filtered detections\")\n",
    "        else:\n",
    "            print(f\"  üìÑ {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  üìÅ {results_dir}\")\n",
    "else:\n",
    "    print(\"  ‚ùå No LLM results generated\")\n",
    "\n",
    "print(\"\\nüìä Evaluation:\")\n",
    "if 'evaluation_dir' in locals() and evaluation_dir.exists():\n",
    "    eval_files = list(evaluation_dir.glob(\"*.csv\")) + list(evaluation_dir.glob(\"*.json\"))\n",
    "    for file in eval_files:\n",
    "        size_kb = file.stat().st_size // 1024\n",
    "        print(f\"  üìÑ {file.name} ({size_kb} KB)\")\n",
    "    print(f\"  üìÅ {evaluation_dir}\")\n",
    "else:\n",
    "    print(\"  ‚ùå No evaluation results generated\")\n",
    "\n",
    "print(\"\\nüí° Full transparency: code snippets, prompts, LLM decisions, metrics\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
