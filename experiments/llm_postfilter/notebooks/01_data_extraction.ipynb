{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5d6f8a",
   "metadata": {},
   "source": [
    "# 📊 LLM Post-Filter Experiment: Data Extraction\n",
    "\n",
    "**Focus**: Extract GLITCH detections from baseline experiments for LLM evaluation.\n",
    "\n",
    "## 🎯 Goals:\n",
    "1. Extract all TP/FP GLITCH detections from Chef and Puppet experiments\n",
    "2. Prepare clean dataset for LLM post-filtering pipeline\n",
    "3. Generate summary statistics for baseline performance\n",
    "\n",
    "## 📁 Data Sources:\n",
    "- **Baseline Results**: Chef and Puppet static analysis experiments\n",
    "- **Target Smells**: Hard-coded secret, Suspicious comment, Weak cryptography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e965a70",
   "metadata": {},
   "source": [
    "## 🔧 Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877082ca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to path to import our llm_postfilter modules\n",
    "project_root = Path.cwd().parent.parent.parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "from llm_postfilter.data_extractor import GLITCHDetectionExtractor\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Initialize the extractor\n",
    "extractor = GLITCHDetectionExtractor(project_root)\n",
    "print(\"✅ Data extractor initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaec477",
   "metadata": {},
   "source": [
    "## 📊 Extract Chef Detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e9df4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"🔍 Extracting Chef detections...\")\n",
    "\n",
    "chef_detections = extractor.extract_detections_for_llm('chef')\n",
    "\n",
    "print(f\"\\n📊 Chef Detection Summary:\")\n",
    "for smell, detections in chef_detections.items():\n",
    "    tp_count = sum(1 for d in detections if d['is_true_positive'])\n",
    "    fp_count = sum(1 for d in detections if not d['is_true_positive'])\n",
    "    print(f\"  {smell}: {len(detections)} total | {tp_count} TP | {fp_count} FP\")\n",
    "\n",
    "# Show example detection structure\n",
    "if chef_detections:\n",
    "    first_smell = next(iter(chef_detections.keys()))\n",
    "    if chef_detections[first_smell]:\n",
    "        print(f\"\\n📝 Example detection structure:\")\n",
    "        example = chef_detections[first_smell][0]\n",
    "        for key, value in example.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  No detections found for {first_smell}\")\n",
    "else:\n",
    "    print(\"\\n❌ No detections extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36256169",
   "metadata": {},
   "source": [
    "## 📊 Extract Puppet Detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043301bc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"🔍 Extracting Puppet detections...\")\n",
    "\n",
    "puppet_detections = extractor.extract_detections_for_llm('puppet')\n",
    "\n",
    "print(f\"\\n📊 Puppet Detection Summary:\")\n",
    "for smell, detections in puppet_detections.items():\n",
    "    tp_count = sum(1 for d in detections if d['is_true_positive'])\n",
    "    fp_count = sum(1 for d in detections if not d['is_true_positive'])\n",
    "    print(f\"  {smell}: {len(detections)} total | {tp_count} TP | {fp_count} FP\")\n",
    "\n",
    "# Cross-tool comparison\n",
    "print(f\"\\n🔄 Chef vs Puppet Comparison:\")\n",
    "all_smells = set(chef_detections.keys()) | set(puppet_detections.keys())\n",
    "for smell in sorted(all_smells):\n",
    "    chef_count = len(chef_detections.get(smell, []))\n",
    "    puppet_count = len(puppet_detections.get(smell, []))\n",
    "    print(f\"  {smell}: Chef={chef_count}, Puppet={puppet_count}\")\n",
    "\n",
    "total_detections = (sum(len(detections) for detections in chef_detections.values()) + \n",
    "                   sum(len(detections) for detections in puppet_detections.values()))\n",
    "print(f\"\\n✅ Total detections extracted: {total_detections}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9e84f",
   "metadata": {},
   "source": [
    "## 💾 Save Detection Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711a9f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"💾 Saving detection dataset...\")\n",
    "\n",
    "output_dir = project_root / \"experiments/llm_postfilter/data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save detection files\n",
    "chef_saved = extractor.save_detections('chef', output_dir)\n",
    "puppet_saved = extractor.save_detections('puppet', output_dir)\n",
    "\n",
    "print(f\"✅ Detection files saved:\")\n",
    "for file_path in sorted(output_dir.glob(\"*_detections.csv\")):\n",
    "    file_size = file_path.stat().st_size\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"  📄 {file_path.name}: {len(df)} detections ({file_size:,} bytes)\")\n",
    "\n",
    "print(f\"\\n🎯 Dataset ready for LLM post-filtering pipeline!\")\n",
    "print(f\"📁 Location: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744a2b9",
   "metadata": {},
   "source": [
    "## 🔍 Extract Code Context for LLM Analysis\n",
    "\n",
    "**Extract ±3 lines of code context around each GLITCH detection for LLM evaluation.**\n",
    "\n",
    "This provides transparent, analyzable code snippets for the LLM post-filtering pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ee8f7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import context extractor\n",
    "from llm_postfilter.context_extractor import CodeContextExtractor\n",
    "\n",
    "# Setup directories\n",
    "context_dir = output_dir / \"with_context\"\n",
    "context_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize context extractor\n",
    "context_extractor = CodeContextExtractor(project_root)\n",
    "\n",
    "# Find detection files to process\n",
    "detection_files = list(output_dir.glob(\"*_detections.csv\"))\n",
    "detection_files = [f for f in detection_files if not f.name.endswith(\"_with_context.csv\") and not f.name.endswith(\"_llm_filtered.csv\")]\n",
    "\n",
    "print(f\"📁 Found {len(detection_files)} detection files:\")\n",
    "for file in detection_files:\n",
    "    df = pd.read_csv(file)\n",
    "    tp_count = df['is_true_positive'].sum()\n",
    "    fp_count = len(df) - tp_count\n",
    "    print(f\"  📄 {file.name}: {len(df)} detections ({tp_count} TP, {fp_count} FP)\")\n",
    "\n",
    "print(f\"\\n🔍 Extracting code context for LLM analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71cbae2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Process each detection file and save context-enhanced versions\n",
    "context_enhanced_files = []\n",
    "context_stats = []\n",
    "\n",
    "for i, detection_file in enumerate(detection_files):\n",
    "    print(f\"\\n🔄 Processing {i+1}/{len(detection_files)}: {detection_file.name}\")\n",
    "    \n",
    "    # Extract context and save enhanced file\n",
    "    enhanced_df = context_extractor.process_and_save_detections(\n",
    "        detection_file, context_dir, context_lines=3\n",
    "    )\n",
    "    \n",
    "    # Track files and stats\n",
    "    base_name = detection_file.stem\n",
    "    context_file = context_dir / f\"{base_name}_with_context.csv\"\n",
    "    context_enhanced_files.append(context_file)\n",
    "    \n",
    "    # Calculate stats\n",
    "    total_detections = len(enhanced_df)\n",
    "    successful_context = enhanced_df['context_success'].sum()\n",
    "    files_found = enhanced_df['file_found'].sum()\n",
    "    \n",
    "    context_stats.append({\n",
    "        'file': detection_file.name,\n",
    "        'total_detections': total_detections,\n",
    "        'files_found': files_found,\n",
    "        'context_extracted': successful_context,\n",
    "        'success_rate': successful_context / total_detections if total_detections > 0 else 0\n",
    "    })\n",
    "    \n",
    "    print(f\"✅ {context_file.name}: {successful_context}/{total_detections} context extracted ({successful_context/total_detections:.1%})\")\n",
    "\n",
    "print(f\"\\n🎯 Context extraction completed for {len(detection_files)} files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5379f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Display context extraction summary\n",
    "print(\"📊 Context Extraction Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "stats_df = pd.DataFrame(context_stats)\n",
    "for _, row in stats_df.iterrows():\n",
    "    print(f\"📄 {row['file']}: {row['context_extracted']}/{row['total_detections']} ({row['success_rate']:.1%}) context extracted\")\n",
    "\n",
    "# Overall statistics\n",
    "total_detections = stats_df['total_detections'].sum()\n",
    "total_context_extracted = stats_df['context_extracted'].sum()\n",
    "overall_success_rate = total_context_extracted / total_detections if total_detections > 0 else 0\n",
    "\n",
    "print(f\"\\n🎯 Overall: {total_context_extracted}/{total_detections} ({overall_success_rate:.1%}) successful\")\n",
    "print(f\"📁 Context files saved: {len(context_enhanced_files)} → {context_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8c369",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Show example context snippet that LLM will analyze\n",
    "print(\"🔍 Example Context Snippet for LLM\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if context_enhanced_files:\n",
    "    example_file = context_enhanced_files[0]\n",
    "    example_df = pd.read_csv(example_file)\n",
    "    successful_detections = example_df[example_df['context_success'] == True]\n",
    "    \n",
    "    if len(successful_detections) > 0:\n",
    "        example = successful_detections.iloc[0]\n",
    "        print(f\"📁 {example_file.name}\")\n",
    "        print(f\"🎯 {example['smell_category']} | TP: {example['is_true_positive']}\")\n",
    "        print(f\"\\n📄 Context Snippet:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(example['context_snippet'])\n",
    "        print(\"-\" * 30)\n",
    "    else:\n",
    "        print(\"❌ No successful context extractions found\")\n",
    "else:\n",
    "    print(\"❌ No context-enhanced files available\")\n",
    "\n",
    "print(f\"\\n💡 All context files saved → {context_dir}\")\n",
    "\n",
    "print(f\"\\n🎯 Data preparation completed!\")\n",
    "print(f\"📁 Detection files: {output_dir}\")\n",
    "print(f\"📁 Context files: {context_dir}\")\n",
    "print(f\"\\n➡️  Next: Run 02_llm_experiment.py for LLM evaluation\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
